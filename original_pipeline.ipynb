{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163c1083",
   "metadata": {},
   "source": [
    "# Original pipeline\n",
    "\n",
    "At some point, fighting with my own pipeline code, I tried again to impelemnt the original pipeline to be able to compare the ground truth to my implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa475b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 17:27:26.138276: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from lib.ClimSim.climsim_utils.data_utils import data_utils\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fe6d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_info = xr.open_dataset(\"data/ClimSim_low-res/ClimSim_low-res_grid-info.nc\")\n",
    "input_mean = xr.open_dataset(\"lib/ClimSim/preprocessing/normalizations/inputs/input_mean.nc\", engine=\"h5netcdf\")\n",
    "input_max = xr.open_dataset(\"lib/ClimSim/preprocessing/normalizations/inputs/input_max.nc\", engine=\"h5netcdf\")\n",
    "input_min = xr.open_dataset(\"lib/ClimSim/preprocessing/normalizations/inputs/input_min.nc\", engine=\"h5netcdf\")\n",
    "output_scale = xr.open_dataset(\"lib/ClimSim/preprocessing/normalizations/outputs/output_scale.nc\", engine=\"h5netcdf\")\n",
    "\n",
    "data = data_utils(\n",
    "    grid_info=grid_info,\n",
    "    input_mean=input_mean,\n",
    "    input_max=input_max,\n",
    "    input_min=input_min,\n",
    "    output_scale=output_scale\n",
    ")\n",
    "\n",
    "data.set_to_v1_vars()\n",
    "data.ml_backend = \"tensorflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2543acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de fihciers d'entrainement : 10890\n"
     ]
    }
   ],
   "source": [
    "data.data_path = \"data/ClimSim_low-res/train/\" \n",
    "data.set_regexps('train', [\"E3SM-MMF.mli.*.nc\"])\n",
    "data.set_stride_sample('train', 1)\n",
    "data.set_filelist(data_split='train')\n",
    "\n",
    "print(\"Nombre de fihciers d'entrainement :\", len(data.get_filelist('train')))\n",
    "tf_records = data.load_ncdata_with_generator(data_split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdfe877b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandre-tonon/anaconda3/envs/pie_env/lib/python3.12/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MLP_PATH = \"lib/ClimSim/baseline_models/MLP/model/backup_phase-7_retrained_models_step2_lot-147_trial_0027.best.h5\"\n",
    "\n",
    "mlp_model = tf.keras.models.load_model(MLP_PATH, compile=False)\n",
    "# mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3681b855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancement de l'évaluation...\n",
      "\n",
      "Variable                  | MAE Moyenne (W/m2)\n",
      "---------------------------------------------\n",
      "ptend_t                   | 2.3972e+00\n",
      "ptend_q0001               | 4.0709e+00\n",
      "cam_out_NETSW             | 1.2766e+01\n",
      "cam_out_FLWDS             | 5.5224e+00\n",
      "cam_out_PRECSC            | 2.9727e+00\n",
      "cam_out_PRECC             | 3.0532e+01\n",
      "cam_out_SOLS              | 7.7381e+00\n",
      "cam_out_SOLL              | 1.0262e+01\n",
      "cam_out_SOLSD             | 4.6384e+00\n",
      "cam_out_SOLLD             | 5.0515e+00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "FEATURES = {\n",
    "    \"features\" :{\n",
    "        \"multilevel\" : [\"in_state_t\", \"in_state_q0001\"],\n",
    "        \"surface\" : [ \"in_state_ps\", 'in_pbuf_SOLIN', 'in_pbuf_LHFLX', 'in_pbuf_SHFLX'],\n",
    "    },  \n",
    "    \"target\" :{\n",
    "        \"tendancies\" : [\"out_ptend_t\", \"out_ptend_q0001\"],\n",
    "        \"surface\" : [\"out_cam_out_NETSW\", \"out_cam_out_FLWDS\", \"out_cam_out_PRECSC\", \"out_cam_out_PRECC\", \"out_cam_out_SOLS\", \"out_cam_out_SOLL\", \"out_cam_out_SOLSD\", \"out_cam_out_SOLLD\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "target_vars = FEATURES[\"target\"][\"tendancies\"] + FEATURES[\"target\"][\"surface\"]\n",
    "\n",
    "stats = {}\n",
    "for var in target_vars:\n",
    "    short_name = re.sub(r'^(in_|out_)', '', var) \n",
    "    size = 60 if var in FEATURES[\"target\"][\"tendancies\"] else 1\n",
    "    stats[short_name] = {\n",
    "        \"ss_res\": np.zeros(size),\n",
    "        \"sum_abs_err\": np.zeros(size),\n",
    "        \"sum_y\": np.zeros(size),\n",
    "        \"sum_y_sq\": np.zeros(size),\n",
    "        \"count\": 0\n",
    "    }\n",
    "\n",
    "# 2. Boucle d'évaluation sur le Dataset TF\n",
    "# On utilise un petit nombre de batches pour ton test (ex: take(5))\n",
    "test_ds = tf_records.batch(10) # 384 = ncol (une grille complète par batch)\n",
    "all_mae = {short_name: [] for short_name in stats.keys()}\n",
    "print(\"Lancement de l'évaluation...\")\n",
    "\n",
    "for x_batch, y_true_batch in test_ds.take(5): \n",
    "    # 1. On récupère les dimensions dynamiquement\n",
    "    # shape sera (10, 384, 124) ou (10, 384, 128)\n",
    "    x_shape = tf.shape(x_batch)\n",
    "    y_shape = tf.shape(y_true_batch)\n",
    "\n",
    "    # -1 dit à TF de calculer automatiquement (10 * 384 = 3840)\n",
    "    x_flat = tf.reshape(x_batch, [-1, 124])\n",
    "    y_flat = tf.reshape(y_true_batch, [-1, 128])\n",
    "\n",
    "    data.input_train = x_flat\n",
    "    data.set_pressure_grid('train')\n",
    "\n",
    "    # 3. Prédiction sur le vecteur aplati\n",
    "    preds_flat = mlp_model.predict(x_flat, verbose=0)\n",
    "\n",
    "    # 5. Conversion et stats\n",
    "    true_dict = data.output_weighting(y_flat.numpy(), data_split='train')\n",
    "    pred_dict = data.output_weighting(preds_flat, data_split='train')\n",
    "    \n",
    "    for var in target_vars:\n",
    "        short_name = re.sub(r'^(out_|in_)', '', var)\n",
    "        \n",
    "        y_t = true_dict[short_name]\n",
    "        y_p = pred_dict[short_name]\n",
    "        \n",
    "        # On appelle ta fonction\n",
    "        # Elle renvoie un vecteur de 60 (niveaux) ou un scalaire\n",
    "        mae_score = data.calc_MAE(y_p, y_t, avg_grid=True)\n",
    "        all_mae[short_name].append(mae_score)\n",
    "\n",
    "# 2. Affichage des moyennes finales\n",
    "print(f\"\\n{'Variable':<25} | {'MAE Moyenne (W/m2)':<15}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for var, scores in all_mae.items():\n",
    "    # Moyenne sur tous les batches, puis moyenne sur les 60 niveaux\n",
    "    final_score = np.mean(np.mean(scores, axis=0))\n",
    "    print(f\"{var:<25} | {final_score:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "010c7a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancement de l'évaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 17:41:19.864041: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 38092800 exceeds 10% of free system memory.\n",
      "2026-02-01 17:41:19.864080: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 39321600 exceeds 10% of free system memory.\n",
      "2026-02-01 17:41:25.148413: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 19660800 exceeds 10% of free system memory.\n",
      "2026-02-01 17:41:40.312767: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 38092800 exceeds 10% of free system memory.\n",
      "2026-02-01 17:41:40.312799: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 39321600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SCORES GLOBAUX (dataset-level) ===\n",
      "Variable                  |   MAE global |    R2 global | % R2 valides\n",
      "----------------------------------------------------------------------\n",
      "ptend_t                   |   2.4911e+00 |   6.5439e-01 |       100.0%\n",
      "ptend_q0001               |   4.2012e+00 |  -2.3460e+03 |        80.0%\n",
      "cam_out_NETSW             |   1.2781e+01 |   9.8903e-01 |       100.0%\n",
      "cam_out_FLWDS             |   5.4429e+00 |   9.9198e-01 |       100.0%\n",
      "cam_out_PRECSC            |   2.8079e+00 |   8.6181e-01 |       100.0%\n",
      "cam_out_PRECC             |   3.2678e+01 |   8.2283e-01 |       100.0%\n",
      "cam_out_SOLS              |   7.6387e+00 |   9.7596e-01 |       100.0%\n",
      "cam_out_SOLL              |   1.0139e+01 |   9.6466e-01 |       100.0%\n",
      "cam_out_SOLSD             |   4.5480e+00 |   9.6528e-01 |       100.0%\n",
      "cam_out_SOLLD             |   4.9300e+00 |   9.0005e-01 |       100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 17:44:58.291840: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "FEATURES = {\n",
    "    \"features\": {\n",
    "        \"multilevel\": [\"in_state_t\", \"in_state_q0001\"],\n",
    "        \"surface\": [\"in_state_ps\", \"in_pbuf_SOLIN\", \"in_pbuf_LHFLX\", \"in_pbuf_SHFLX\"],\n",
    "    },\n",
    "    \"target\": {\n",
    "        \"tendancies\": [\"out_ptend_t\", \"out_ptend_q0001\"],\n",
    "        \"surface\": [\n",
    "            \"out_cam_out_NETSW\", \"out_cam_out_FLWDS\", \"out_cam_out_PRECSC\",\n",
    "            \"out_cam_out_PRECC\", \"out_cam_out_SOLS\", \"out_cam_out_SOLL\",\n",
    "            \"out_cam_out_SOLSD\", \"out_cam_out_SOLLD\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "target_vars = FEATURES[\"target\"][\"tendancies\"] + FEATURES[\"target\"][\"surface\"]\n",
    "\n",
    "def short_name(var: str) -> str:\n",
    "    return re.sub(r'^(in_|out_)', '', var)\n",
    "\n",
    "def var_size(var: str) -> int:\n",
    "    return 60 if var in FEATURES[\"target\"][\"tendancies\"] else 1\n",
    "\n",
    "def to_2d(y: np.ndarray, L: int) -> np.ndarray:\n",
    "    \"\"\"Force y à shape (N, L) pour accumulations.\"\"\"\n",
    "    y = np.asarray(y)\n",
    "    if L == 1:\n",
    "        return y.reshape(-1, 1)\n",
    "    if y.shape[-1] != L:\n",
    "        raise ValueError(f\"Attendu dernière dim={L}, reçu shape {y.shape}\")\n",
    "    return y.reshape(-1, L)\n",
    "\n",
    "SS_TOT_EPS = 1e-10  # seuil variance pour R² \"valide\"\n",
    "\n",
    "# ---------------------------\n",
    "# Accumulateurs globaux\n",
    "# ---------------------------\n",
    "stats = {}\n",
    "for var in target_vars:\n",
    "    sname = short_name(var)\n",
    "    L = var_size(var)\n",
    "    stats[sname] = {\n",
    "        \"sum_abs_err\": np.zeros(L, dtype=np.float64),\n",
    "        \"ss_res\":      np.zeros(L, dtype=np.float64),\n",
    "        \"sum_y\":       np.zeros(L, dtype=np.float64),\n",
    "        \"sum_y_sq\":    np.zeros(L, dtype=np.float64),\n",
    "        \"count\":       0\n",
    "    }\n",
    "\n",
    "# Pour tes moyennes \"comme avant\"\n",
    "all_mae = {sname: [] for sname in stats.keys()}\n",
    "\n",
    "# Optionnel: scores R² par batch (moyenne ensuite)\n",
    "all_r2_batch = {sname: [] for sname in stats.keys()}\n",
    "\n",
    "print(\"Lancement de l'évaluation...\")\n",
    "\n",
    "# Dataset\n",
    "test_ds = tf_records.batch(100)  # ex: (10, 384, 124) / (10, 384, 128)\n",
    "\n",
    "# ---------------------------\n",
    "# Boucle\n",
    "# ---------------------------\n",
    "for x_batch, y_true_batch in test_ds.take(10):\n",
    "\n",
    "    # ⚠️ Tu avais des dims hardcodées (124,128). Ici on reste compatible :\n",
    "    in_dim  = int(x_batch.shape[-1])\n",
    "    out_dim = int(y_true_batch.shape[-1])\n",
    "\n",
    "    x_flat = tf.reshape(x_batch, [-1, in_dim])\n",
    "    y_flat = tf.reshape(y_true_batch, [-1, out_dim])\n",
    "\n",
    "    data.input_train = x_flat\n",
    "    data.set_pressure_grid('train')\n",
    "\n",
    "    preds_flat = mlp_model.predict(x_flat, verbose=0)\n",
    "\n",
    "    # dictionnaires pondérés\n",
    "    true_dict = data.output_weighting(y_flat.numpy(), data_split='train')\n",
    "    pred_dict = data.output_weighting(preds_flat,      data_split='train')\n",
    "\n",
    "    for var in target_vars:\n",
    "        sname = short_name(var)\n",
    "        L = var_size(var)\n",
    "\n",
    "        y_t = true_dict[sname]\n",
    "        y_p = pred_dict[sname]\n",
    "\n",
    "        # ---------------------------\n",
    "        # 1) MAE \"comme avant\" (batch-wise)\n",
    "        # ---------------------------\n",
    "        mae_score = data.calc_MAE(y_p, y_t, avg_grid=True)\n",
    "        all_mae[sname].append(np.asarray(mae_score))\n",
    "\n",
    "        # ---------------------------\n",
    "        # 2) Accumulation pour GLOBAL (MAE + R²)\n",
    "        # ---------------------------\n",
    "        y_t2 = to_2d(y_t, L)  # (N, L)\n",
    "        y_p2 = to_2d(y_p, L)  # (N, L)\n",
    "        err = y_t2 - y_p2\n",
    "        N = y_t2.shape[0]\n",
    "\n",
    "        stats[sname][\"sum_abs_err\"] += np.sum(np.abs(err), axis=0)\n",
    "        stats[sname][\"ss_res\"]      += np.sum(err**2, axis=0)\n",
    "        stats[sname][\"sum_y\"]       += np.sum(y_t2, axis=0)\n",
    "        stats[sname][\"sum_y_sq\"]    += np.sum(y_t2**2, axis=0)\n",
    "        stats[sname][\"count\"]       += N\n",
    "\n",
    "        # ---------------------------\n",
    "        # 3) Optionnel: R² batch-wise (même formule)\n",
    "        # ---------------------------\n",
    "        y_mean_b = np.mean(y_t2, axis=0)\n",
    "        ss_tot_b = np.sum((y_t2 - y_mean_b) ** 2, axis=0)\n",
    "        ss_res_b = np.sum(err ** 2, axis=0)\n",
    "\n",
    "        r2_b = np.full(L, np.nan, dtype=np.float64)\n",
    "        mask_b = ss_tot_b > SS_TOT_EPS\n",
    "        r2_b[mask_b] = 1.0 - (ss_res_b[mask_b] / ss_tot_b[mask_b])\n",
    "\n",
    "        all_r2_batch[sname].append(r2_b)\n",
    "\n",
    "# ---------------------------\n",
    "# Affichages\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "# B) Scores globaux (MAE global + R² global robuste)\n",
    "print(f\"\\n=== SCORES GLOBAUX (dataset-level) ===\")\n",
    "print(f\"{'Variable':<25} | {'MAE global':>12} | {'R2 global':>12} | {'% R2 valides':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for sname, s in stats.items():\n",
    "    N = s[\"count\"]\n",
    "    if N == 0:\n",
    "        print(f\"{sname:<25} | {'NA':>12} | {'NA':>12} | {'NA':>12}\")\n",
    "        continue\n",
    "\n",
    "    mae_per_level = s[\"sum_abs_err\"] / N\n",
    "\n",
    "    y_mean = s[\"sum_y\"] / N\n",
    "    ss_tot = s[\"sum_y_sq\"] - N * (y_mean ** 2)\n",
    "    ss_res = s[\"ss_res\"]\n",
    "\n",
    "    r2_per_level = np.full_like(ss_tot, np.nan, dtype=np.float64)\n",
    "    mask = ss_tot > SS_TOT_EPS\n",
    "    r2_per_level[mask] = 1.0 - (ss_res[mask] / ss_tot[mask])\n",
    "\n",
    "    mae_global = float(np.nanmean(mae_per_level))\n",
    "    r2_global  = float(np.nanmean(r2_per_level))\n",
    "    pct_valid  = 100.0 * float(np.mean(mask))\n",
    "\n",
    "    print(f\"{sname:<25} | {mae_global:12.4e} | {r2_global:12.4e} | {pct_valid:11.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pie_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
