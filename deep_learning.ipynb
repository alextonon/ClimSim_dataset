{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51a5194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import torch\n",
    "from functools import reduce \n",
    "\n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LOW_RES_SAMPLE_PATH = \"data/ClimSim_low-res/train/\"\n",
    "LOW_RES_GRID_PATH = \"data/ClimSim_low-res/ClimSim_low-res_grid-info.nc\"\n",
    "ZARR_PATH = \"data/ClimSim_low-res.zarr\"\n",
    "NORM_PATH = \"lib/ClimSim/preprocessing/normalizations/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "546cf04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimSimMLP(nn.Module):\n",
    "    def __init__(self, input_dim=556, output_tendancies_dim=120, output_surface_dim=8):\n",
    "        super(ClimSimMLP, self).__init__()\n",
    "        \n",
    "        # Hidden Layers: [768, 640, 512, 640, 640]\n",
    "        self.layer1 = nn.Linear(input_dim, 768)\n",
    "        self.layer2 = nn.Linear(768, 640)\n",
    "\n",
    "        self.layer3 = nn.Linear(640, 512)\n",
    "        self.layer4 = nn.Linear(512, 640)\n",
    "        self.layer5 = nn.Linear(640, 640)\n",
    "        \n",
    "\n",
    "        self.last_hidden = nn.Linear(640, 128)\n",
    "        \n",
    "        # --- Output Heads ---\n",
    "        self.head_tendencies = nn.Linear(128, output_tendancies_dim)\n",
    "        # self.head_surface = nn.Linear(128, output_surface_dim)\n",
    "        \n",
    "        # LeakyReLU alpha=0.15\n",
    "        self.activation = nn.LeakyReLU(0.15)\n",
    "        \n",
    "        for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the 5 main hidden layers\n",
    "        x = self.activation(self.layer1(x))\n",
    "        x = self.activation(self.layer2(x))\n",
    "        x = self.activation(self.layer3(x))\n",
    "        x = self.activation(self.layer4(x))\n",
    "        x = self.activation(self.layer5(x))\n",
    "        \n",
    "        # Pass through the fixed 128 layer\n",
    "        x = self.activation(self.last_hidden(x))\n",
    "        \n",
    "        # Output 1: Tendencies (Linear activation)\n",
    "        out_linear = self.head_tendencies(x)\n",
    "        \n",
    "        # Output 2: Surface variables (ReLU activation)\n",
    "        # out_relu = F.relu(self.head_surface(x))\n",
    "        \n",
    "        # Concatenate along the feature dimension (dim=1)\n",
    "        return out_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4595e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils.data import Dataset, Subset\n",
    "import re\n",
    "\n",
    "from lib import data\n",
    "\n",
    "\n",
    "class ClimSimBase:\n",
    "    def __init__(self, zarr_path, grid_path, norm_path, features, num_latlon = 384, normalize=True):\n",
    "        self.ds = xr.open_zarr(zarr_path, chunks=None)\n",
    "        self.features = features\n",
    "        self.features_list = self.__get_features__()\n",
    "        self.normalize_flag = normalize\n",
    "        self.num_latlon = num_latlon\n",
    "        self.grid = xr.open_dataset(grid_path, engine=\"netcdf4\")\n",
    "        \n",
    "        self.input_mean = xr.open_dataset(os.path.join(norm_path, \"inputs/input_mean.nc\"), engine=\"netcdf4\")\n",
    "        self.input_std = xr.open_dataset(os.path.join(norm_path, \"inputs/input_std.nc\"), engine=\"netcdf4\")\n",
    "        self.input_max = xr.open_dataset(os.path.join(norm_path, \"inputs/input_max.nc\"), engine=\"netcdf4\")\n",
    "        self.input_min = xr.open_dataset(os.path.join(norm_path, \"inputs/input_min.nc\"), engine=\"netcdf4\")\n",
    "        self.output_scale = xr.open_dataset(os.path.join(norm_path, \"outputs/output_scale.nc\"), engine=\"netcdf4\")\n",
    "\n",
    "        self.grid['area_wgt'] = self.grid['area']/self.grid['area'].mean(dim = 'ncol')\n",
    "        self.area_wgt = self.grid['area_wgt'].values\n",
    "\n",
    "        self.input_vars = [v for v in self.features_list if 'in' in v]\n",
    "        self.output_vars = [v for v in self.features_list if 'out' in v]\n",
    "\n",
    "        self.grav    = 9.80616    # acceleration of gravity ~ m/s^2\n",
    "        self.cp      = 1.00464e3  # specific heat of dry air   ~ J/kg/K\n",
    "        self.lv      = 2.501e6    # latent heat of evaporation ~ J/kg\n",
    "        self.lf      = 3.337e5    # latent heat of fusion      ~ J/kg\n",
    "        self.lsub    = self.lv + self.lf    # latent heat of sublimation ~ J/kg\n",
    "        self.rho_air = 101325/(6.02214e26*1.38065e-23/28.966)/273.15 # density of dry air at STP  ~ kg/m^3\n",
    "        self.rho_h20 = 1.e3       # density of fresh water     ~ kg/m^ 3\n",
    "\n",
    "        self.target_energy_conv = {'ptend_t':self.cp,\n",
    "                            'ptend_q0001':self.lv,\n",
    "                            'ptend_q0002':self.lv,\n",
    "                            'ptend_q0003':self.lv,\n",
    "                            'ptend_qn':self.lv,\n",
    "                            'ptend_wind': None,\n",
    "                            'cam_out_NETSW':1.,\n",
    "                            'cam_out_FLWDS':1.,\n",
    "                            'cam_out_PRECSC':self.lv*self.rho_h20,\n",
    "                            'cam_out_PRECC':self.lv*self.rho_h20,\n",
    "                            'cam_out_SOLS':1.,\n",
    "                            'cam_out_SOLL':1.,\n",
    "                            'cam_out_SOLSD':1.,\n",
    "                            'cam_out_SOLLD':1.\n",
    "                            }\n",
    "        \n",
    "        \n",
    "        self.dp = None \n",
    "        self.pressure_grid = None\n",
    "\n",
    "    def __get_features__(self):\n",
    "        feat = np.concatenate([self.features[\"features\"][\"multilevel\"], self.features[\"features\"][\"surface\"]])\n",
    "        target = np.concatenate([self.features[\"target\"][\"tendancies\"], self.features[\"target\"][\"surface\"]])\n",
    "        return np.concatenate([feat, target])\n",
    "\n",
    "    def _prepare_data(self, idx):\n",
    "        x = self.process_list(self.input_vars, idx, is_input=True)\n",
    "        y = self.process_list(self.output_vars, idx, is_input=False)\n",
    "        return x, y\n",
    "\n",
    "    def process_list(self, vars_list, idx, is_input=True):\n",
    "        out_list = []\n",
    "        n_geo = self.num_latlon # 384\n",
    "\n",
    "        for var in vars_list:\n",
    "            if \"ptend\" in var:\n",
    "                # Cette fonction doit renvoyer du (Time, 384, 60)\n",
    "                data = self._calculate_tendency_on_fly(var, idx)\n",
    "                if data.ndim == 2: # Si (384, 60)\n",
    "                    data = data[np.newaxis, :, :]\n",
    "            \n",
    "            else:\n",
    "                da = self.ds[var].isel(sample=idx)\n",
    "                \n",
    "                # Redressement par nom de dimension Xarray\n",
    "                if 'lev' in da.dims:\n",
    "                    if \"sample\" in da.dims:\n",
    "                        data = da.transpose('sample', 'ncol', 'lev').values\n",
    "                    else:\n",
    "                        data = da.transpose('ncol', 'lev').values[np.newaxis, :, :]\n",
    "                else:\n",
    "                    if \"sample\" in da.dims:\n",
    "                        # Surface : (Time, 384)\n",
    "                        data = da.values[:, :, np.newaxis]  # Ajouter une dimension lev=1\n",
    "                    else:\n",
    "                        # Surface : (Time, 384) -> (Time, 384, 1)\n",
    "                        data = da.values[np.newaxis, :, np.newaxis]\n",
    "            \n",
    "            # 2. Normalisation (Maintenant data est garanti (N, 384, L))\n",
    "            data = self._normalize_var(data, var, is_input=is_input)\n",
    "            out_list.append(data.astype(np.float32))\n",
    "\n",
    "        # 3. Concaténation et aplatissement\n",
    "        combined = np.concatenate(out_list, axis=-1)\n",
    "        return combined.reshape(-1, combined.shape[-1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ds.dims['sample']\n",
    "\n",
    "    def _calculate_tendency_on_fly(self, var, idx):\n",
    "        dt = 1200\n",
    "        mapping = {\n",
    "            'out_ptend_t': ('out_state_t', 'in_state_t'),\n",
    "            'out_ptend_q0001': ('out_state_q0001', 'in_state_q0001'),\n",
    "            'out_ptend_u': ('out_state_u', 'in_state_u'),\n",
    "            'out_ptend_v': ('out_state_v', 'in_state_v'),\n",
    "        }\n",
    "        out_v, in_v = mapping[var]\n",
    "\n",
    "        v_final = self.ds[out_v].isel(sample=idx)\n",
    "        v_init  = self.ds[in_v].isel(sample=idx)\n",
    "\n",
    "        # Fonction utilitaire: remettre en ordre (sample?, ncol, lev) si sample existe\n",
    "        def to_array(da):\n",
    "            dims = da.dims\n",
    "\n",
    "            if 'ncol' not in dims or 'lev' not in dims:\n",
    "                raise ValueError(f\"{da.name}: dims inattendues {dims}, attendu ncol & lev\")\n",
    "\n",
    "            # Cas slice -> dims contiennent sample\n",
    "            if 'sample' in dims:\n",
    "                da = da.transpose('sample', 'ncol', 'lev')\n",
    "                return da.values  # (sample, ncol, lev)\n",
    "\n",
    "            # Cas int -> dims (lev, ncol) ou (ncol, lev)\n",
    "            da = da.transpose('ncol', 'lev')\n",
    "            return da.values[None, ...]  # (1, ncol, lev)\n",
    "\n",
    "        vf = to_array(v_final)\n",
    "        vi = to_array(v_init)\n",
    "\n",
    "        return (vf - vi) / dt  # (time, ncol, lev)\n",
    "\n",
    "\n",
    "    \n",
    "    def _normalize_var(self, data, var_name, is_input=True):\n",
    "        # data est (N, 384, L) où L est 1 ou 60\n",
    "        short_name = re.sub(r'^(in_|out_)', '', var_name)\n",
    "        \n",
    "        if is_input:\n",
    "            m = self.input_mean[short_name].values     # (L,)\n",
    "            diff = (self.input_max[short_name].values - self.input_min[short_name].values) # (L,)\n",
    "            \n",
    "            # On redimensionne les stats en (1, 1, L) pour s'aligner sur data (N, 384, L)\n",
    "            m = m.reshape(1, 1, -1)\n",
    "            diff = diff.reshape(1, 1, -1)\n",
    "            \n",
    "            return (data - m) / (diff + 1e-15)\n",
    "        else:\n",
    "            scale = self.output_scale[short_name].values # (L,)\n",
    "            return data * scale.reshape(1, 1, -1)\n",
    "            \n",
    "    def set_pressure_grid(self, input_data):\n",
    "        '''\n",
    "        Calcule la grille de pression 3D à partir de state_ps.\n",
    "        Code directement issu de ClimSim original.\n",
    "        '''\n",
    "        self.ps_index = self._find_ps_index(self.features)\n",
    "        state_ps = input_data[:, self.ps_index]\n",
    "        if self.normalize_flag:\n",
    "            state_ps = state_ps * (self.input_max['state_ps'].values - self.input_min['state_ps'].values) + self.input_mean['state_ps'].values\n",
    "        \n",
    "        state_ps = state_ps.reshape(-1, self.num_latlon)\n",
    "\n",
    "        p1 = (self.grid['P0'] * self.grid['hyai']).values[:, None, None]\n",
    "        p2 = self.grid['hybi'].values[:, None, None] * state_ps[None, :, :]\n",
    "        \n",
    "        self.pressure_grid = p1 + p2\n",
    "        self.dp = (self.pressure_grid[1:61] - self.pressure_grid[0:60]).transpose((1, 2, 0))\n",
    "    \n",
    "    \n",
    "    def denormalize_output(self, y_pred):\n",
    "        \"\"\"Dénormalise les prédictions.\"\"\"\n",
    "\n",
    "        full_scale_vector = [] # To vectorize we generate the full scale vector first\n",
    "        for var in self.output_vars:\n",
    "            short_name = re.sub(r'^(in_|out_)', '', var)\n",
    "            scale = self.output_scale[short_name].values\n",
    "            \n",
    "            if 'ptend' in var:\n",
    "                dim_size = 60\n",
    "            elif 'lev' in self.ds[var].dims:\n",
    "                dim_size = self.ds[var].sizes['lev']\n",
    "            else:\n",
    "                dim_size = 1\n",
    "            \n",
    "            if np.isscalar(scale) or scale.size == 1:\n",
    "                scale_expanded = np.full(dim_size, scale)\n",
    "            else:\n",
    "                scale_expanded = scale\n",
    "                \n",
    "            full_scale_vector.append(scale_expanded)\n",
    "        \n",
    "        full_scale_vector = np.concatenate(full_scale_vector)\n",
    "\n",
    "        y_denorm = y_pred / (full_scale_vector + 1e-8) \n",
    "\n",
    "        return (y_denorm).astype(np.float32)\n",
    "    \n",
    "    def calc_MAE(self, pred, target, avg_grid = True):\n",
    "        '''\n",
    "        calculate 'globally averaged' mean absolute error \n",
    "        for vertically-resolved variables, shape should be time x grid x level\n",
    "        for scalars, shape should be time x grid\n",
    "\n",
    "        returns vector of length level or 1\n",
    "        '''\n",
    "        assert pred.shape[1] == self.num_latlon\n",
    "        assert pred.shape == target.shape\n",
    "        mae = np.abs(pred - target).mean(axis = 0)\n",
    "        if avg_grid:\n",
    "            return mae.mean(axis = 0) # we decided to average globally at end\n",
    "        else:\n",
    "            return mae\n",
    "    \n",
    "    def calc_RMSE(self, pred, target, avg_grid = True):\n",
    "        '''\n",
    "        calculate 'globally averaged' root mean squared error \n",
    "        for vertically-resolved variables, shape should be time x grid x level\n",
    "        for scalars, shape should be time x grid\n",
    "\n",
    "        returns vector of length level or 1\n",
    "        '''\n",
    "        assert pred.shape[1] == self.num_latlon\n",
    "        assert pred.shape == target.shape\n",
    "        sq_diff = (pred - target)**2\n",
    "        rmse = np.sqrt(sq_diff.mean(axis = 0)) # mean over time\n",
    "        if avg_grid:\n",
    "            return rmse.mean(axis = 0) # we decided to separately average globally at end\n",
    "        else:\n",
    "            return rmse\n",
    "\n",
    "    def calc_R2(self, pred, target, avg_grid = True):\n",
    "        '''\n",
    "        calculate 'globally averaged' R-squared\n",
    "        for vertically-resolved variables, input shape should be time x grid x level\n",
    "        for scalars, input shape should be time x grid\n",
    "\n",
    "        returns vector of length level or 1\n",
    "        '''\n",
    "        assert pred.shape[1] == self.num_latlon\n",
    "        assert pred.shape == target.shape\n",
    "        sq_diff = (pred - target)**2\n",
    "        tss_time = (target - target.mean(axis = 0)[np.newaxis, ...])**2 # mean over time\n",
    "        r_squared = 1 - sq_diff.sum(axis = 0)/tss_time.sum(axis = 0) # sum over time\n",
    "        if avg_grid:\n",
    "            return r_squared.mean(axis = 0) # we decided to separately average globally at end\n",
    "        else:\n",
    "            return r_squared\n",
    "    \n",
    "    def output_weighting(self, output):\n",
    "        num_samples = output.shape[0]\n",
    "        n_geo = self.num_latlon\n",
    "        n_time = num_samples // n_geo\n",
    "        \n",
    "        # On reste sur le dictionnaire de base\n",
    "        var_dict = {}\n",
    "        \n",
    "        # On récupère dp/g (le poids vertical)\n",
    "        # Assure-toi que self.dp est bien calculé avant !\n",
    "        dp_weight = self.dp / self.grav # (Time, 384, 60)\n",
    "\n",
    "        # Exemple pour ptend_t (indices 0:60)\n",
    "        ptend_t = output[:, 0:60].reshape(n_time, n_geo, 60)\n",
    "        # 1. Dénormalisation (revenir aux unités physiques : K/s)\n",
    "        ptend_t /= self.output_scale['ptend_t'].values[None, None, :]\n",
    "        # 2. Conversion en W/m2 (poids vertical * chaleur spécifique)\n",
    "        var_dict['ptend_t'] = ptend_t * dp_weight * self.cp\n",
    "        \n",
    "        # Exemple pour ptend_q0001 (indices 60:120)\n",
    "        ptend_q = output[:, 60:120].reshape(n_time, n_geo, 60)\n",
    "        ptend_q /= self.output_scale['ptend_q0001'].values[None, None, :]\n",
    "        var_dict['ptend_q0001'] = ptend_q * dp_weight * self.lv\n",
    "        \n",
    "        # Pour les variables de surface (indices 120:128)\n",
    "        # Elles sont déjà en W/m2 après dé-scaling, pas besoin de dp/g\n",
    "        surface_vars = ['cam_out_NETSW', 'cam_out_FLWDS', 'cam_out_PRECSC', \n",
    "                        'cam_out_PRECC', 'cam_out_SOLS', 'cam_out_SOLL', \n",
    "                        'cam_out_SOLSD', 'cam_out_SOLLD']\n",
    "        \n",
    "        for i, var in enumerate(surface_vars):\n",
    "            idx = 120 + i\n",
    "            val = output[:, idx].reshape(n_time, n_geo)\n",
    "            # Dé-scaling uniquement\n",
    "            val /= self.output_scale[var].values\n",
    "            var_dict[var] = val\n",
    "\n",
    "        return var_dict\n",
    "    \n",
    "    def _find_ps_index(self, features_dict):\n",
    "        \"\"\"\n",
    "        Calcule l'index de départ de 'in_state_ps' dans le vecteur d'entrée plat.\n",
    "        Prend en compte que chaque variable multilevel occupe 60 colonnes.\n",
    "        \"\"\"\n",
    "        current_index = 0\n",
    "        \n",
    "        # 1. Parcourir les variables multi-niveaux (60 niveaux chacune)\n",
    "        for var in features_dict[\"features\"][\"multilevel\"]:\n",
    "            if var == \"in_state_ps\":\n",
    "                return current_index\n",
    "            current_index += 60\n",
    "            \n",
    "        # 2. Parcourir les variables de surface (1 niveau chacune)\n",
    "        for var in features_dict[\"features\"][\"surface\"]:\n",
    "            if var == \"in_state_ps\":\n",
    "                return current_index\n",
    "            current_index += 1\n",
    "            \n",
    "        raise ValueError(\"Variable 'in_state_ps' non trouvée dans le dictionnaire FEATURES.\")\n",
    "\n",
    "class ClimSimPyTorch(ClimSimBase, Dataset):\n",
    "    def __getitem__(self, idx):\n",
    "        x_np, y_np = self._prepare_data(idx)\n",
    "        return torch.from_numpy(x_np), torch.from_numpy(y_np)\n",
    "\n",
    "    # On peut remettre ta méthode de split ici\n",
    "    def train_test_split(self, test_size=0.2, seed=42, shuffle=True):\n",
    "        n = len(self)\n",
    "        indices = np.arange(n)\n",
    "        if shuffle:\n",
    "            rng = np.random.default_rng(seed)\n",
    "            rng.shuffle(indices)\n",
    "        split = int((1 - test_size) * n)\n",
    "        return Subset(self, indices[:split]), Subset(self, indices[split:])\n",
    "    \n",
    "    def get_models_dims(self, variables_dict):\n",
    "        features_tend = variables_dict[\"features\"][\"multilevel\"]\n",
    "        features_surf = variables_dict[\"features\"][\"surface\"]\n",
    "        \n",
    "        target_tend = variables_dict[\"target\"][\"tendancies\"]\n",
    "        target_surf = variables_dict[\"target\"][\"surface\"]\n",
    "\n",
    "        def get_var_dim(var):\n",
    "            # 1. Gérer les variables virtuelles (tendances calculées)\n",
    "            if 'ptend' in var:\n",
    "                # On mappe vers la variable d'état pour connaître la dimension 'lev'\n",
    "                # ex: out_ptend_t -> out_state_t\n",
    "                source_var = var.replace('ptend', 'state')\n",
    "                return self.ds[source_var].sizes['lev']\n",
    "            \n",
    "            # 2. Gérer les variables réelles présentes dans le Zarr\n",
    "            if 'lev' in self.ds[var].dims:\n",
    "                return self.ds[var].sizes['lev']\n",
    "            \n",
    "            # 3. Variables de surface (scalaires)\n",
    "            return 1\n",
    "\n",
    "        in_tend_dim = sum([get_var_dim(var) for var in features_tend])\n",
    "        in_surf_dim = len(features_surf)\n",
    "        \n",
    "        out_tend_dim = sum([get_var_dim(var) for var in target_tend])\n",
    "        out_surf_dim = len(target_surf)\n",
    "\n",
    "        return {\n",
    "            \"input_total\": in_tend_dim + in_surf_dim,\n",
    "            \"output_tendancies\": out_tend_dim,\n",
    "            \"output_surface\": out_surf_dim\n",
    "        }\n",
    "    \n",
    "        \n",
    "    def _normalize_var(self, data, var_name, is_input=True):\n",
    "        \"\"\"Applique la normalisation selon que la variable est 3D ou de surface.\"\"\"\n",
    "        if not self.normalize_flag:\n",
    "            return data\n",
    "\n",
    "        short_name = re.sub(r'^(in_|out_)', '', var_name)\n",
    "\n",
    "        if is_input:\n",
    "            # Récupération des valeurs (L,) ou scalaire\n",
    "            m = self.input_mean[short_name].values\n",
    "            s = self.input_std[short_name].values\n",
    "            \n",
    "            # Redimensionnement en (1, 1, L) pour broadcast sur (N, 384, L)\n",
    "            # -1 détecte automatiquement si c'est 1 (surface) ou 60 (profil)\n",
    "            m_norm = m.reshape(1, 1, -1)\n",
    "            s_norm = s.reshape(1, 1, -1)\n",
    "            \n",
    "            return (data - m_norm) / (s_norm + 1e-8)\n",
    "        \n",
    "        else:\n",
    "            # Pour l'output (Target), on multiplie par le scale\n",
    "            scale = self.output_scale[short_name].values\n",
    "            \n",
    "            # Redimensionnement identique (1, 1, L)\n",
    "            scale_norm = scale.reshape(1, 1, -1)\n",
    "            \n",
    "            return data * scale_norm\n",
    "        \n",
    "    def get_batch_for_pytorch(self, idx, input_dim, output_dim):\n",
    "        x_np, y_np = self._prepare_data(idx)\n",
    "\n",
    "        x_np = x_np.reshape(-1, input_dim)\n",
    "        y_np = y_np.reshape(-1, output_dim)\n",
    "\n",
    "        return torch.from_numpy(x_np).float(), torch.from_numpy(y_np).float()          \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class ClimSimKeras(ClimSimBase):\n",
    "    def get_batch_for_keras(self, idx, input_dim, output_dim):\n",
    "        x_np, y_np = self._prepare_data(idx)\n",
    "\n",
    "        x_np = x_np.reshape(-1, input_dim)\n",
    "        y_np = y_np.reshape(-1, output_dim)\n",
    "        return x_np, y_np\n",
    "    \n",
    "    def get_sample_number(self):\n",
    "        return self.ds.dims['sample']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d44ae816",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ZARR_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      2\u001b[39m N_EPOCHS = \u001b[32m10\u001b[39m\n\u001b[32m      4\u001b[39m FEATURES = {\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m :{\n\u001b[32m      6\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultilevel\u001b[39m\u001b[33m\"\u001b[39m : [\u001b[33m\"\u001b[39m\u001b[33min_state_t\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33min_state_q0001\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     }\n\u001b[32m     13\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m dataset = ClimSimPyTorch(\u001b[43mZARR_PATH\u001b[49m, LOW_RES_GRID_PATH, NORM_PATH, FEATURES, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     16\u001b[39m model_dims = dataset.get_models_dims(FEATURES)\n\u001b[32m     18\u001b[39m model = ClimSimMLP(input_dim=model_dims[\u001b[33m\"\u001b[39m\u001b[33minput_total\u001b[39m\u001b[33m\"\u001b[39m], output_tendancies_dim=model_dims[\u001b[33m\"\u001b[39m\u001b[33moutput_tendancies\u001b[39m\u001b[33m\"\u001b[39m], output_surface_dim=model_dims[\u001b[33m\"\u001b[39m\u001b[33moutput_surface\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'ZARR_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 3072\n",
    "N_EPOCHS = 10\n",
    "\n",
    "FEATURES = {\n",
    "    \"features\" :{\n",
    "        \"multilevel\" : [\"in_state_t\", \"in_state_q0001\"],\n",
    "        \"surface\" : [\"in_state_ps\", \"in_pbuf_LHFLX\", \"in_pbuf_SHFLX\", \"in_pbuf_SOLIN\"],\n",
    "    },  \n",
    "    \"target\" :{\n",
    "        \"tendancies\" : [\"out_ptend_t\", \"out_ptend_q0001\"],\n",
    "        \"surface\" : []\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset = ClimSimPyTorch(ZARR_PATH, LOW_RES_GRID_PATH, NORM_PATH, FEATURES, normalize=True)\n",
    "model_dims = dataset.get_models_dims(FEATURES)\n",
    "\n",
    "model = ClimSimMLP(input_dim=model_dims[\"input_total\"], output_tendancies_dim=model_dims[\"output_tendancies\"], output_surface_dim=model_dims[\"output_surface\"])\n",
    "optimizer = torch.optim.RAdam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef62f675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4881/2688885185.py:106: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  return self.ds.dims['sample']\n"
     ]
    }
   ],
   "source": [
    "train, test = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f9892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1 [00:01<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_total\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_tendancies\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_surface\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     val_loss = evaluate_model(\n\u001b[32m     12\u001b[39m         model, \n\u001b[32m     13\u001b[39m         test_loader, \n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m         output_dim=model_dims[\u001b[33m\"\u001b[39m\u001b[33moutput_tendancies\u001b[39m\u001b[33m\"\u001b[39m] + model_dims[\u001b[33m\"\u001b[39m\u001b[33moutput_surface\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     18\u001b[39m         )\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion, device, input_dim, output_dim)\u001b[39m\n\u001b[32m     68\u001b[39m total_samples = \u001b[32m0\u001b[39m\n\u001b[32m     70\u001b[39m pbar = tqdm(dataloader, desc=\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m, unit=\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Allow flattening for MLP\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1448\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1447\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1451\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1412\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1408\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1409\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1411\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1412\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1413\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1414\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1243\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1231\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1232\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1241\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1244\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1245\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1246\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1247\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1248\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/multiprocessing/connection.py:1136\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1133\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train_one_epoch(\n",
    "        model, \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        device=\"cpu\",\n",
    "        input_dim=model_dims[\"input_total\"],\n",
    "        output_dim=model_dims[\"output_tendancies\"] + model_dims[\"output_surface\"],\n",
    "        )\n",
    "    val_loss = evaluate_model(\n",
    "        model, \n",
    "        test_loader, \n",
    "        criterion, \n",
    "        device=\"cpu\",\n",
    "        input_dim=model_dims[\"input_total\"],\n",
    "        output_dim=model_dims[\"output_tendancies\"] + model_dims[\"output_surface\"],\n",
    "        )\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    model_path = f\"climsim_mlp_epoch{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6996eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4881/1604313159.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"models/climsim_mlp_epoch10.pth\", map_location=torch.device('cpu'))\n",
      "/tmp/ipykernel_4881/2688885185.py:287: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  val /= scale[None, None, :] if is_prof else scale\n",
      "/tmp/ipykernel_4881/2688885185.py:290: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  if is_prof: val *= dp\n",
      "/tmp/ipykernel_4881/2688885185.py:293: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  val *= self.area_wgt[None, :, None] if is_prof else self.area_wgt[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variable                  | MAE (W/m2)   | RMSE (W/m2)  | R2 Global \n",
      "---------------------------------------------------------------------------\n",
      "out_ptend_t               | 1.1484e+02| -77.8710\n",
      "out_ptend_q0001           | 1.1205e+02| -3291028786516.5146\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "models_dims = dataset.get_models_dims(FEATURES)\n",
    "\n",
    "state_dict = torch.load(\"models/climsim_mlp_epoch10.pth\", map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "target_vars = FEATURES[\"target\"][\"tendancies\"] + FEATURES[\"target\"][\"surface\"]\n",
    "input_size = models_dims[\"input_total\"]\n",
    "output_size = models_dims[\"output_tendancies\"] + models_dims[\"output_surface\"]\n",
    "\n",
    "# 1. Initialisation des accumulateurs\n",
    "stats = {}\n",
    "for var in target_vars:\n",
    "    short_name = re.sub(r'^(in_|out_)', '', var)\n",
    "    size = 60 if var in FEATURES[\"target\"][\"tendancies\"] else 1\n",
    "    stats[short_name] = {\n",
    "        \"ss_res\": np.zeros(size),      # Somme des carrés (pour RMSE et R2)\n",
    "        \"sum_abs_err\": np.zeros(size), # Somme des erreurs absolues (pour MAE)\n",
    "        \"sum_y\": np.zeros(size),       # Somme de y (pour variance R2)\n",
    "        \"sum_y_sq\": np.zeros(size),    # Somme de y^2 (pour variance R2)\n",
    "        \"count\": 0\n",
    "    }\n",
    "\n",
    "# 2. Boucle de test (Batch par Batch)\n",
    "for i in range(100):\n",
    "    x_batch, y_true_batch = dataset.get_batch_for_pytorch(i, input_size, output_size)\n",
    "    dataset.set_pressure_grid(x_batch.numpy()) # Mise à jour de dp pour ce batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds_batch = model(x_batch)\n",
    "        preds_batch = preds_batch.cpu().numpy()\n",
    "    \n",
    "\n",
    "    # Récupération des dictionnaires physiques {var: (Time, 384, L)}\n",
    "    true_dict = dataset.output_weighting(y_true_batch)\n",
    "    pred_dict = dataset.output_weighting(preds_batch)\n",
    "\n",
    "    for var in true_dict.keys():\n",
    "        if torch.is_tensor(true_dict[var]):\n",
    "            true_dict[var] = true_dict[var].detach().cpu().numpy()\n",
    "        if torch.is_tensor(pred_dict[var]):\n",
    "            pred_dict[var] = pred_dict[var].detach().cpu().numpy()\n",
    "\n",
    "    \n",
    "    \n",
    "    for var in target_vars:\n",
    "        short_name = re.sub(r'^(in_|out_)', '', var)\n",
    "        y_t = true_dict[short_name] \n",
    "        y_p = pred_dict[short_name]\n",
    "        \n",
    "        # Différences\n",
    "        diff = y_t - y_p\n",
    "        \n",
    "        # Accumulation (on réduit sur Time et Grille : axes 0 et 1)\n",
    "        stats[short_name][\"ss_res\"] += np.sum(diff**2, axis=(0, 1))\n",
    "        stats[short_name][\"sum_abs_err\"] += np.sum(np.abs(diff), axis=(0, 1))\n",
    "        stats[short_name][\"sum_y\"] += np.sum(y_t, axis=(0, 1))\n",
    "        stats[short_name][\"sum_y_sq\"] += np.sum(y_t**2, axis=(0, 1))\n",
    "        stats[short_name][\"count\"] += y_t.shape[0] * y_t.shape[1]\n",
    "\n",
    "# 3. Calcul final et Affichage\n",
    "print(f\"\\n{'Variable':<25} | {'MAE (W/m2)':<12} | {'RMSE (W/m2)':<12} | {'R2 Global':<10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for var in target_vars:\n",
    "    short_name = re.sub(r'^(in_|out_)', '', var)\n",
    "    s = stats[short_name]\n",
    "    n = s[\"count\"]\n",
    "    \n",
    "    # MAE par niveau et moyenne\n",
    "    mae_levels = s[\"sum_abs_err\"] / n\n",
    "    final_mae = np.mean(mae_levels)\n",
    "    \n",
    "    # R2 par niveau et moyenne\n",
    "    ss_tot = s[\"sum_y_sq\"] - (s[\"sum_y\"]**2 / n)\n",
    "    r2_levels = 1 - (s[\"ss_res\"] / (ss_tot + 1e-15))\n",
    "    final_r2 = np.mean(r2_levels)\n",
    "    \n",
    "    print(f\"{var:<25} | {final_mae:.4e}| {final_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d641c95",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m all_targets = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Move to device\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Flatten the ncol dimension into the batch dimension for the MLP\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# inputs shape: [Batch, 384, Input_Dim] -> [Batch*384, Input_Dim]\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1448\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1447\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1451\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1412\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1408\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1409\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1411\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1412\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1413\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1414\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1243\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1231\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1232\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1241\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1244\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1245\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1246\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1247\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1248\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/multiprocessing/connection.py:1136\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1133\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        # Move to device\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Flatten the ncol dimension into the batch dimension for the MLP\n",
    "        # inputs shape: [Batch, 384, Input_Dim] -> [Batch*384, Input_Dim]\n",
    "        inputs = inputs.view(-1, inputs.shape[-1])\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        all_preds.append(outputs.cpu().numpy())\n",
    "        all_targets.append(targets.view(-1, targets.shape[-1]).cpu().numpy())\n",
    "\n",
    "# Combine everything\n",
    "final_preds = np.concatenate(all_preds, axis=0)\n",
    "final_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "# IMPORTANT: Denormalize BEFORE calculating physical metrics\n",
    "preds_phys = dataset.denormalize_output(final_preds)\n",
    "targets_phys = dataset.denormalize_output(final_targets)\n",
    "\n",
    "# Use your built-in MAE/RMSE\n",
    "# Note: You might need to reshape back to (Time, 384, Var) for calc_MAE \n",
    "# if your functions expect the spatial dimension separated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pie_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
