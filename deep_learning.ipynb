{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51a5194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import torch\n",
    "from functools import reduce \n",
    "\n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LOW_RES_SAMPLE_PATH = \"data/ClimSim_low-res/train/\"\n",
    "LOW_RES_GRID_PATH = \"data/ClimSim_low-res/ClimSim_low-res_grid-info.nc\"\n",
    "ZARR_PATH = \"data/ClimSim_low-res.zarr\"\n",
    "NORM_PATH = \"ClimSim/preprocessing/normalizations/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "546cf04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimSimMLP(nn.Module):\n",
    "    def __init__(self, input_dim=556, output_tendancies_dim=120, output_surface_dim=8):\n",
    "        super(ClimSimMLP, self).__init__()\n",
    "        \n",
    "        # Hidden Layers: [768, 640, 512, 640, 640]\n",
    "        self.layer1 = nn.Linear(input_dim, 768)\n",
    "        self.layer2 = nn.Linear(768, 640)\n",
    "        self.layer3 = nn.Linear(640, 512)\n",
    "        self.layer4 = nn.Linear(512, 640)\n",
    "        self.layer5 = nn.Linear(640, 640)\n",
    "        \n",
    "\n",
    "        self.last_hidden = nn.Linear(640, 128)\n",
    "        \n",
    "        # --- Output Heads ---\n",
    "        self.head_tendencies = nn.Linear(128, output_tendancies_dim)\n",
    "        self.head_surface = nn.Linear(128, output_surface_dim)\n",
    "        \n",
    "        # LeakyReLU alpha=0.15\n",
    "        self.activation = nn.LeakyReLU(0.15)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the 5 main hidden layers\n",
    "        x = self.activation(self.layer1(x))\n",
    "        x = self.activation(self.layer2(x))\n",
    "        x = self.activation(self.layer3(x))\n",
    "        x = self.activation(self.layer4(x))\n",
    "        x = self.activation(self.layer5(x))\n",
    "        \n",
    "        # Pass through the fixed 128 layer\n",
    "        x = self.activation(self.last_hidden(x))\n",
    "        \n",
    "        # Output 1: Tendencies (Linear activation)\n",
    "        out_linear = self.head_tendencies(x)\n",
    "        \n",
    "        # Output 2: Surface variables (ReLU activation)\n",
    "        out_relu = F.relu(self.head_surface(x))\n",
    "        \n",
    "        # Concatenate along the feature dimension (dim=1)\n",
    "        return torch.cat([out_linear, out_relu], dim=1)\n",
    "\n",
    "        return out_linear\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, criterion, device, input_dim, output_dim):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        inputs = inputs.view(-1, input_dim)  # Allow flattening for MLP\n",
    "        targets = targets.view(-1, output_dim) \n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        batch_size = inputs.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "    average_loss = total_loss / total_samples\n",
    "    return average_loss\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, input_dim, output_dim):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\", unit=\"batch\")\n",
    "\n",
    "    for inputs, targets in pbar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        inputs = inputs.view(-1, input_dim)  # Allow flattening for MLP\n",
    "        targets = targets.view(-1, output_dim) \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_size = inputs.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "        \n",
    "        # Update progress bar description with current loss\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.6f}\"})\n",
    "\n",
    "    return total_loss / total_samples\n",
    "\n",
    "# Pour reproduire le gagnant :\n",
    "n_layers = 5\n",
    "units = [768, 640, 512, 640, 640]\n",
    "hp_act = 'leakyrelu'\n",
    "hp_optimizer = 'RAdam'\n",
    "hp_batch_size = 3072\n",
    "\n",
    "vars_mli = ['state_t','state_q0001','state_ps','pbuf_SOLIN', 'pbuf_LHFLX', 'pbuf_SHFLX']\n",
    "vars_mlo = ['ptend_t','ptend_q0001','cam_out_NETSW','cam_out_FLWDS','cam_out_PRECSC',\n",
    "            'cam_out_PRECC','cam_out_SOLS','cam_out_SOLL','cam_out_SOLSD','cam_out_SOLLD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "312aa5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from scipy import stats\n",
    "import torch\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ClimSimZarrDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                zarr_path, \n",
    "                grid_path,\n",
    "                norm_path,\n",
    "                features,\n",
    "                normalize=True,\n",
    "                transform=None):\n",
    "        self.zarr_path = zarr_path\n",
    "\n",
    "        self.features = features\n",
    "        self.features_list = self.__get_features__()\n",
    "\n",
    "        self.ds = xr.open_zarr(zarr_path, chunks=\"auto\")\n",
    "        self.setup_tendencies()\n",
    "        self.ds = self.ds[self.features_list]\n",
    "\n",
    "        self.grid = xr.open_dataset(grid_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.input_mean = xr.open_dataset(norm_path + \"inputs/input_mean.nc\")\n",
    "        self.input_std = xr.open_dataset(norm_path + \"inputs/input_std.nc\")\n",
    "        self.output_scale = xr.open_dataset(norm_path + \"outputs/output_scale.nc\")\n",
    "        \n",
    "        self.length = self.ds.dims['sample']\n",
    "        \n",
    "        self.input_vars = [v for v in self.ds.data_vars if 'in' in v]\n",
    "        self.output_vars = [v for v in self.ds.data_vars if 'out' in v]\n",
    "\n",
    "        if normalize:\n",
    "            self.normalize()\n",
    "\n",
    "    def setup_tendencies(self):    \n",
    "        timestep = 1200 # secondes\n",
    "        \n",
    "        self.ds['out_ptend_t'] = (self.ds['out_state_t'] - self.ds['in_state_t']) / timestep\n",
    "        self.ds['out_ptend_q0001'] = (self.ds['out_state_q0001'] - self.ds['in_state_q0001']) / timestep\n",
    "        self.ds['out_ptend_u'] = (self.ds['out_state_u'] - self.ds['in_state_u']) / timestep # U tendency [m/s/s]\n",
    "        self.ds['out_ptend_v'] = (self.ds['out_state_v'] - self.ds['in_state_v']) / timestep # V tendency [m/s/s]\n",
    "\n",
    "        self.target_list = [\"out_ptend_t\", \"out_ptend_q0001\", \"out_ptend_u\", \"out_ptend_v\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __get_features__(self):\n",
    "        feat = np.concat([self.features[\"features\"][\"tendancies\"], self.features[\"features\"][\"surface\"]])\n",
    "        target = np.concat([self.features[\"target\"][\"tendancies\"], self.features[\"target\"][\"surface\"]])\n",
    "        return np.concat([feat, target])\n",
    "\n",
    "    def __getitem__(self, idx, normalize=True):\n",
    "        def prepare_data(vars_list):\n",
    "            output_list = []\n",
    "            for var in vars_list:\n",
    "                data = self.ds[var][idx].values # Faster than isel ?\n",
    "                \n",
    "                if data.ndim == 1: # Variable de surface (ncol,) -> (ncol, 1)\n",
    "                    data = data[:, np.newaxis]\n",
    "                else: # Variable 3D (nlev, ncol) -> (ncol, nlev)\n",
    "                    data = data.T\n",
    "                \n",
    "                output_list.append(data)\n",
    "            \n",
    "            return np.concatenate(output_list, axis=1).astype(np.float32)\n",
    "\n",
    "        x_np = prepare_data(self.input_vars) # (384, 246)\n",
    "        y_np = prepare_data(self.output_vars) # (384, 61)\n",
    "\n",
    "        x = torch.from_numpy(x_np)\n",
    "        y = torch.from_numpy(y_np)\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    def get_models_dims(self, variables_dict):\n",
    "        features_tend = variables_dict[\"features\"][\"tendancies\"]\n",
    "        features_surf = variables_dict[\"features\"][\"surface\"]\n",
    "        \n",
    "        target_tend = variables_dict[\"target\"][\"tendancies\"]\n",
    "        target_surf = variables_dict[\"target\"][\"surface\"]\n",
    "\n",
    "        def get_var_dim(var):\n",
    "            if 'lev' in self.ds[var].dims:\n",
    "                return self.ds[var].sizes['lev']\n",
    "            return 1\n",
    "\n",
    "        in_tend_dim = sum([get_var_dim(var) for var in features_tend])\n",
    "        in_surf_dim = len(features_surf)\n",
    "        \n",
    "        out_tend_dim = sum([get_var_dim(var) for var in target_tend])\n",
    "        out_surf_dim = len(target_surf)\n",
    "\n",
    "        return {\n",
    "            \"input_total\": in_tend_dim + in_surf_dim,\n",
    "            \"output_tendancies\": out_tend_dim,\n",
    "            \"output_surface\": out_surf_dim\n",
    "        }\n",
    "        \n",
    "    def train_test_split(self, test_size=0.2, seed=42, shuffle=True):\n",
    "        \"\"\"\n",
    "        Split the dataset into train and test subsets without loading any data.\n",
    "        \"\"\"\n",
    "        n = len(self)\n",
    "        indices = np.arange(n)\n",
    "\n",
    "        if shuffle:\n",
    "            rng = np.random.default_rng(seed)\n",
    "            rng.shuffle(indices)\n",
    "\n",
    "        split = int((1 - test_size) * n)\n",
    "\n",
    "        train_indices = indices[:split]\n",
    "        test_indices  = indices[split:]\n",
    "\n",
    "        return (\n",
    "            Subset(self, train_indices),\n",
    "            Subset(self, test_indices)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def normalize(self):\n",
    "        for var in self.input_vars:\n",
    "            var_name = var.replace(\"in_\", \"\", 1)\n",
    "            self.ds[var_name] = (self.ds[var] - self.input_mean[var_name]) / self.input_std[var_name]\n",
    "        \n",
    "        for var in self.output_vars:\n",
    "            var_name = var.replace(\"out_\", \"\", 1)\n",
    "            self.ds[var_name] = self.ds[var] * self.output_scale[var_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c4595e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils.data import Dataset, Subset\n",
    "import re\n",
    "\n",
    "\n",
    "class ClimSimBase:\n",
    "    def __init__(self, zarr_path, grid_path, norm_path, features, normalize=True):\n",
    "        self.ds = xr.open_zarr(zarr_path, chunks=None)\n",
    "        self.features = features\n",
    "        self.features_list = self.__get_features__()\n",
    "        self.normalize_flag = normalize\n",
    "\n",
    "        self.grid = xr.open_dataset(grid_path)\n",
    "        \n",
    "        self.input_mean = xr.open_dataset(f\"{norm_path}inputs/input_mean.nc\")\n",
    "        self.input_std = xr.open_dataset(f\"{norm_path}inputs/input_std.nc\")\n",
    "        self.output_scale = xr.open_dataset(f\"{norm_path}outputs/output_scale.nc\")\n",
    "\n",
    "        self.input_vars = [v for v in self.features_list if 'in' in v]\n",
    "        self.output_vars = [v for v in self.features_list if 'out' in v]\n",
    "\n",
    "    def __get_features__(self):\n",
    "        feat = np.concatenate([self.features[\"features\"][\"tendancies\"], self.features[\"features\"][\"surface\"]])\n",
    "        target = np.concatenate([self.features[\"target\"][\"tendancies\"], self.features[\"target\"][\"surface\"]])\n",
    "        return np.concatenate([feat, target])\n",
    "\n",
    "    def _prepare_data(self, idx):\n",
    "        # On passe idx explicitement à process_list\n",
    "        x = self.process_list(self.input_vars, idx, is_input=True)\n",
    "        y = self.process_list(self.output_vars, idx, is_input=False)\n",
    "        return x, y\n",
    "\n",
    "    def process_list(self, vars_list, idx, is_input=True):\n",
    "        out_list = []\n",
    "        for var in vars_list:\n",
    "            # Récupération\n",
    "            if \"ptend\" in var:\n",
    "                data = self._calculate_tendency_on_fly(var, idx)\n",
    "            else:\n",
    "                # Utiliser .isel() est plus \"xarray-style\" et sécurisé\n",
    "                data = self.ds[var].isel(sample=idx).values\n",
    "            \n",
    "            # Normalisation\n",
    "            data = self._normalize_var(data, var, is_input=is_input)\n",
    "\n",
    "            # Gestion des dimensions : (ncol, nlev) -> ici ncol est implicitement 1 par idx\n",
    "            # On veut un vecteur plat pour concaténer à la fin\n",
    "            if data.ndim == 0: # Scalaire\n",
    "                data = np.array([data])\n",
    "            \n",
    "            out_list.append(data.flatten())\n",
    "            \n",
    "        return np.concatenate(out_list).astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ds.dims['sample']\n",
    "\n",
    "    def _calculate_tendency_on_fly(self, var, idx):\n",
    "        \"\"\"Calcule la tendance uniquement pour l'échantillon demandé\"\"\"\n",
    "        dt = 1200\n",
    "        mapping = {\n",
    "            'out_ptend_t': ('out_state_t', 'in_state_t'),\n",
    "            'out_ptend_q0001': ('out_state_q0001', 'in_state_q0001'),\n",
    "            'out_ptend_u': ('out_state_u', 'in_state_u'),\n",
    "            'out_ptend_v': ('out_state_v', 'in_state_v'),\n",
    "        }\n",
    "        out_v, in_v = mapping[var]\n",
    "        return (self.ds[out_v][idx].values - self.ds[in_v][idx].values) / dt\n",
    "\n",
    "    def _normalize_var(self, data, var_name, is_input=True):\n",
    "        \"\"\"Applique la normalisation selon que la variable est 3D ou de surface.\"\"\"\n",
    "        if not self.normalize_flag:\n",
    "            return data\n",
    "\n",
    "        short_name = re.sub(r'^(in_|out_)', '', var_name)\n",
    "\n",
    "        if is_input:\n",
    "            m = self.input_mean[short_name].values\n",
    "            s = self.input_std[short_name].values\n",
    "            \n",
    "            m_norm = m[:, np.newaxis] if m.ndim > 0 else m\n",
    "            s_norm = s[:, np.newaxis] if s.ndim > 0 else s\n",
    "            \n",
    "            return (data - m_norm) / (s_norm + 1e-8)\n",
    "        else:\n",
    "            scale = self.output_scale[short_name].values\n",
    "            scale_norm = scale[:, np.newaxis] if scale.ndim > 0 else scale\n",
    "            return data * scale_norm\n",
    "    \n",
    "\n",
    "class ClimSimPyTorch(ClimSimBase, Dataset):\n",
    "    def __getitem__(self, idx):\n",
    "        x_np, y_np = self._prepare_data(idx)\n",
    "        return torch.from_numpy(x_np), torch.from_numpy(y_np)\n",
    "\n",
    "    # On peut remettre ta méthode de split ici\n",
    "    def train_test_split(self, test_size=0.2, seed=42, shuffle=True):\n",
    "        n = len(self)\n",
    "        indices = np.arange(n)\n",
    "        if shuffle:\n",
    "            rng = np.random.default_rng(seed)\n",
    "            rng.shuffle(indices)\n",
    "        split = int((1 - test_size) * n)\n",
    "        return Subset(self, indices[:split]), Subset(self, indices[split:])\n",
    "    \n",
    "    def get_models_dims(self, variables_dict):\n",
    "        features_tend = variables_dict[\"features\"][\"tendancies\"]\n",
    "        features_surf = variables_dict[\"features\"][\"surface\"]\n",
    "        \n",
    "        target_tend = variables_dict[\"target\"][\"tendancies\"]\n",
    "        target_surf = variables_dict[\"target\"][\"surface\"]\n",
    "\n",
    "        def get_var_dim(var):\n",
    "            # 1. Gérer les variables virtuelles (tendances calculées)\n",
    "            if 'ptend' in var:\n",
    "                # On mappe vers la variable d'état pour connaître la dimension 'lev'\n",
    "                # ex: out_ptend_t -> out_state_t\n",
    "                source_var = var.replace('ptend', 'state')\n",
    "                return self.ds[source_var].sizes['lev']\n",
    "            \n",
    "            # 2. Gérer les variables réelles présentes dans le Zarr\n",
    "            if 'lev' in self.ds[var].dims:\n",
    "                return self.ds[var].sizes['lev']\n",
    "            \n",
    "            # 3. Variables de surface (scalaires)\n",
    "            return 1\n",
    "\n",
    "        in_tend_dim = sum([get_var_dim(var) for var in features_tend])\n",
    "        in_surf_dim = len(features_surf)\n",
    "        \n",
    "        out_tend_dim = sum([get_var_dim(var) for var in target_tend])\n",
    "        out_surf_dim = len(target_surf)\n",
    "\n",
    "        return {\n",
    "            \"input_total\": in_tend_dim + in_surf_dim,\n",
    "            \"output_tendancies\": out_tend_dim,\n",
    "            \"output_surface\": out_surf_dim\n",
    "        }\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d44ae816",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3072\n",
    "N_EPOCHS = 10\n",
    "\n",
    "FEATURES = {\n",
    "    \"features\" :{\n",
    "        \"tendancies\" : [\"in_state_t\", \"in_state_q0001\", \"in_state_ps\"],\n",
    "        \"surface\" : [\"in_pbuf_LHFLX\", \"in_pbuf_SHFLX\", \"in_pbuf_SOLIN\"],\n",
    "    },  \n",
    "    \"target\" :{\n",
    "        \"tendancies\" : [\"out_ptend_t\", \"out_ptend_q0001\"],\n",
    "        \"surface\" : [\"out_cam_out_NETSW\", \"out_cam_out_FLWDS\", \"out_cam_out_PRECSC\", \"out_cam_out_PRECC\", \"out_cam_out_SOLS\", \"out_cam_out_SOLL\", \"out_cam_out_SOLSD\", \"out_cam_out_SOLLD\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset = ClimSimPyTorch(ZARR_PATH, LOW_RES_GRID_PATH, NORM_PATH, FEATURES, normalize=True)\n",
    "model_dims = dataset.get_models_dims(FEATURES)\n",
    "\n",
    "model = ClimSimMLP(input_dim=model_dims[\"input_total\"], output_tendancies_dim=model_dims[\"output_tendancies\"], output_surface_dim=model_dims[\"output_surface\"])\n",
    "optimizer = torch.optim.RAdam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef62f675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5137/4112968753.py:58: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  return self.ds.dims['sample']\n"
     ]
    }
   ],
   "source": [
    "train, test = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e27f9892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14 [00:43<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_total\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_tendancies\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_surface\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     val_loss = evaluate_model(\n\u001b[32m     12\u001b[39m         model, \n\u001b[32m     13\u001b[39m         test_loader, \n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m         output_dim=model_dims[\u001b[33m\"\u001b[39m\u001b[33moutput_tendancies\u001b[39m\u001b[33m\"\u001b[39m] + model_dims[\u001b[33m\"\u001b[39m\u001b[33moutput_surface\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     18\u001b[39m         )\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion, device, input_dim, output_dim)\u001b[39m\n\u001b[32m     68\u001b[39m total_samples = \u001b[32m0\u001b[39m\n\u001b[32m     70\u001b[39m pbar = tqdm(dataloader, desc=\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m, unit=\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Allow flattening for MLP\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1482\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1444\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1440\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1441\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1444\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1445\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1446\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1275\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1263\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1264\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1278\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1279\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1280\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/multiprocessing/connection.py:1136\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1133\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train_one_epoch(\n",
    "        model, \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        device=\"cpu\",\n",
    "        input_dim=model_dims[\"input_total\"],\n",
    "        output_dim=model_dims[\"output_tendancies\"] + model_dims[\"output_surface\"],\n",
    "        )\n",
    "    val_loss = evaluate_model(\n",
    "        model, \n",
    "        test_loader, \n",
    "        criterion, \n",
    "        device=\"cpu\",\n",
    "        input_dim=model_dims[\"input_total\"],\n",
    "        output_dim=model_dims[\"output_tendancies\"] + model_dims[\"output_surface\"],\n",
    "        )\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    model_path = f\"climsim_mlp_epoch{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca71173",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/alexandre-tonon/deep-learning/ClimSim_dataset/ClimSim_dataset/ClimSim/preprocessing/normalizations/inputs/input_mean.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/xarray/backends/file_manager.py:219\u001b[39m, in \u001b[36mCachingFileManager._acquire_with_cache_info\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/xarray/backends/lru_cache.py:56\u001b[39m, in \u001b[36mLRUCache.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mself\u001b[39m._cache.move_to_end(key)\n",
      "\u001b[31mKeyError\u001b[39m: [<class 'netCDF4._netCDF4.Dataset'>, ('/home/alexandre-tonon/deep-learning/ClimSim_dataset/ClimSim_dataset/ClimSim/preprocessing/normalizations/inputs/input_mean.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '1e247027-91f4-494f-8a77-e5f4e4799ddf']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m input_nc = \u001b[43mxr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mClimSim_dataset/ClimSim/preprocessing/normalizations/inputs/input_mean.nc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/xarray/backends/api.py:606\u001b[39m, in \u001b[36mopen_dataset\u001b[39m\u001b[34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, create_default_indexes, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m decoders = _resolve_decoders_kwargs(\n\u001b[32m    595\u001b[39m     decode_cf,\n\u001b[32m    596\u001b[39m     open_backend_dataset_parameters=backend.open_dataset_parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    602\u001b[39m     decode_coords=decode_coords,\n\u001b[32m    603\u001b[39m )\n\u001b[32m    605\u001b[39m overwrite_encoded_chunks = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33moverwrite_encoded_chunks\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m backend_ds = \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    612\u001b[39m ds = _dataset_from_backend_dataset(\n\u001b[32m    613\u001b[39m     backend_ds,\n\u001b[32m    614\u001b[39m     filename_or_obj,\n\u001b[32m   (...)\u001b[39m\u001b[32m    625\u001b[39m     **kwargs,\n\u001b[32m    626\u001b[39m )\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:767\u001b[39m, in \u001b[36mNetCDF4BackendEntrypoint.open_dataset\u001b[39m\u001b[34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, auto_complex, lock, autoclose)\u001b[39m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen_dataset\u001b[39m(\n\u001b[32m    746\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    747\u001b[39m     filename_or_obj: T_PathFileOrDataStore,\n\u001b[32m   (...)\u001b[39m\u001b[32m    764\u001b[39m     autoclose=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    765\u001b[39m ) -> Dataset:\n\u001b[32m    766\u001b[39m     filename_or_obj = _normalize_path(filename_or_obj)\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m     store = \u001b[43mNetCDF4DataStore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    780\u001b[39m     store_entrypoint = StoreBackendEntrypoint()\n\u001b[32m    781\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:525\u001b[39m, in \u001b[36mNetCDF4DataStore.open\u001b[39m\u001b[34m(cls, filename, mode, format, group, clobber, diskless, persist, auto_complex, lock, lock_maker, autoclose)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    522\u001b[39m     manager = CachingFileManager(\n\u001b[32m    523\u001b[39m         netCDF4.Dataset, filename, mode=mode, kwargs=kwargs\n\u001b[32m    524\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:429\u001b[39m, in \u001b[36mNetCDF4DataStore.__init__\u001b[39m\u001b[34m(self, manager, group, mode, lock, autoclose)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28mself\u001b[39m._group = group\n\u001b[32m    428\u001b[39m \u001b[38;5;28mself\u001b[39m._mode = mode\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28mself\u001b[39m.format = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mds\u001b[49m.data_model\n\u001b[32m    430\u001b[39m \u001b[38;5;28mself\u001b[39m._filename = \u001b[38;5;28mself\u001b[39m.ds.filepath()\n\u001b[32m    431\u001b[39m \u001b[38;5;28mself\u001b[39m.is_remote = is_remote_uri(\u001b[38;5;28mself\u001b[39m._filename)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:534\u001b[39m, in \u001b[36mNetCDF4DataStore.ds\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:528\u001b[39m, in \u001b[36mNetCDF4DataStore._acquire\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43mds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_nc4_require_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/xarray/backends/file_manager.py:207\u001b[39m, in \u001b[36mCachingFileManager.acquire_context\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> Iterator[T_File]:\n\u001b[32m    206\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     file, cached = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    209\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pie_env/lib/python3.12/site-packages/xarray/backends/file_manager.py:225\u001b[39m, in \u001b[36mCachingFileManager._acquire_with_cache_info\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    223\u001b[39m     kwargs = kwargs.copy()\n\u001b[32m    224\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._mode\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mode == \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28mself\u001b[39m._mode = \u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/netCDF4/_netCDF4.pyx:2521\u001b[39m, in \u001b[36mnetCDF4._netCDF4.Dataset.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/netCDF4/_netCDF4.pyx:2158\u001b[39m, in \u001b[36mnetCDF4._netCDF4._ensure_nc_success\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/alexandre-tonon/deep-learning/ClimSim_dataset/ClimSim_dataset/ClimSim/preprocessing/normalizations/inputs/input_mean.nc'"
     ]
    }
   ],
   "source": [
    "input_nc = xr.open_dataset(\"ClimSim/preprocessing/normalizations/inputs/input_mean.nc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pie_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
