{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "821e660f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 11:07:36.239467: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-28 11:07:36.682090: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-28 11:07:36.684281: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-28 11:07:37.926526: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"retrained_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input (InputLayer)          [(None, 124)]                0         []                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 768)                  96000     ['input[0][0]']               \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, 768)                  0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 640)                  492160    ['leaky_re_lu[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 640)                  0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 512)                  328192    ['leaky_re_lu_1[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 512)                  0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 640)                  328320    ['leaky_re_lu_2[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 640)                  0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 640)                  410240    ['leaky_re_lu_3[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 640)                  0         ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 128)                  82048     ['leaky_re_lu_4[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 128)                  0         ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 120)                  15480     ['leaky_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 8)                    1032      ['leaky_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 128)                  0         ['dense_6[0][0]',             \n",
      "                                                                     'dense_7[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1753472 (6.69 MB)\n",
      "Trainable params: 1753472 (6.69 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 11:07:38.906249: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2026-01-28 11:07:38.906275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: alexandre-tonon-XPS-15-9570\n",
      "2026-01-28 11:07:38.906281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: alexandre-tonon-XPS-15-9570\n",
      "2026-01-28 11:07:38.906329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 535.274.2\n",
      "2026-01-28 11:07:38.906348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 535.274.2\n",
      "2026-01-28 11:07:38.906354: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 535.274.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "CNN_PATH = \"ClimSim/baseline_models/CNN/model/\"\n",
    "MLP_PATH = \"ClimSim/baseline_models/MLP/model/backup_phase-7_retrained_models_step2_lot-147_trial_0027.best.h5\"\n",
    "\n",
    "# cnn_model = tf.keras.layers.TFSMLayer(CNN_PATH, call_endpoint=\"serving_default\")\n",
    "mlp_model = tf.keras.models.load_model(MLP_PATH, compile=False)\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f9e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils.data import Dataset, Subset\n",
    "import re\n",
    "\n",
    "\n",
    "class ClimSimBase:\n",
    "    def __init__(self, zarr_path, grid_path, norm_path, features, normalize=True):\n",
    "        self.ds = xr.open_zarr(zarr_path, chunks=\"auto\")\n",
    "        self.features = features\n",
    "        self.normalize_flag = normalize\n",
    "        \n",
    "        self.setup_tendencies()\n",
    "        self.features_list = self.__get_features__()\n",
    "        self.ds = self.ds[self.features_list]\n",
    "\n",
    "        self.grid = xr.open_dataset(grid_path)\n",
    "        \n",
    "        self.input_mean = xr.open_dataset(f\"{norm_path}inputs/input_mean.nc\")\n",
    "        self.input_std = xr.open_dataset(f\"{norm_path}inputs/input_std.nc\")\n",
    "        self.output_scale = xr.open_dataset(f\"{norm_path}outputs/output_scale.nc\")\n",
    "\n",
    "        self.input_vars = [v for v in self.features_list if 'in' in v]\n",
    "        self.output_vars = [v for v in self.features_list if 'out' in v]\n",
    "\n",
    "    def setup_tendencies(self):    \n",
    "        timestep = 1200 \n",
    "        self.ds['out_ptend_t'] = (self.ds['out_state_t'] - self.ds['in_state_t']) / timestep\n",
    "        self.ds['out_ptend_q0001'] = (self.ds['out_state_q0001'] - self.ds['in_state_q0001']) / timestep\n",
    "        self.ds['out_ptend_u'] = (self.ds['out_state_u'] - self.ds['in_state_u']) / timestep\n",
    "        self.ds['out_ptend_v'] = (self.ds['out_state_v'] - self.ds['in_state_v']) / timestep\n",
    "\n",
    "    def __get_features__(self):\n",
    "        feat = np.concatenate([self.features[\"features\"][\"tendancies\"], self.features[\"features\"][\"surface\"]])\n",
    "        target = np.concatenate([self.features[\"target\"][\"tendancies\"], self.features[\"target\"][\"surface\"]])\n",
    "        return np.concatenate([feat, target])\n",
    "\n",
    "    def _prepare_data(self, idx):\n",
    "        \"\"\"La méthode 'cœur' qui extrait et formate\"\"\"\n",
    "        def process_list(vars_list, is_input=True):\n",
    "            out_list = []\n",
    "            for var in vars_list:\n",
    "                data = self.ds[var][idx].values\n",
    "                short_name = re.sub(r'^(in_|out_)', '', var)\n",
    "                \n",
    "                if self.normalize_flag:\n",
    "                    if is_input:\n",
    "                        data = (data - self.input_mean[short_name].values) / (self.input_std[short_name].values + 1e-8)\n",
    "                    else:\n",
    "                        data = data * self.output_scale[short_name].values\n",
    "\n",
    "                # Gestion 1D (surface) vs 3D (lev)\n",
    "                data = data[:, np.newaxis] if data.ndim == 1 else data.T\n",
    "                out_list.append(data)\n",
    "            return np.concatenate(out_list, axis=1).astype(np.float32)\n",
    "\n",
    "        return process_list(self.input_vars, True), process_list(self.output_vars, False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ds.dims['sample']\n",
    "    \n",
    "\n",
    "class ClimSimPyTorch(ClimSimBase, Dataset):\n",
    "    def __getitem__(self, idx):\n",
    "        x_np, y_np = self._prepare_data(idx)\n",
    "        return torch.from_numpy(x_np), torch.from_numpy(y_np)\n",
    "\n",
    "    # On peut remettre ta méthode de split ici\n",
    "    def train_test_split(self, test_size=0.2, seed=42, shuffle=True):\n",
    "        n = len(self)\n",
    "        indices = np.arange(n)\n",
    "        if shuffle:\n",
    "            rng = np.random.default_rng(seed)\n",
    "            rng.shuffle(indices)\n",
    "        split = int((1 - test_size) * n)\n",
    "        return Subset(self, indices[:split]), Subset(self, indices[split:])\n",
    "    \n",
    "    def get_models_dims(self, variables_dict):\n",
    "        features_tend = variables_dict[\"features\"][\"tendancies\"]\n",
    "        features_surf = variables_dict[\"features\"][\"surface\"]\n",
    "        \n",
    "        target_tend = variables_dict[\"target\"][\"tendancies\"]\n",
    "        target_surf = variables_dict[\"target\"][\"surface\"]\n",
    "\n",
    "        def get_var_dim(var):\n",
    "            if 'lev' in self.ds[var].dims:\n",
    "                return self.ds[var].sizes['lev']\n",
    "            return 1\n",
    "\n",
    "        in_tend_dim = sum([get_var_dim(var) for var in features_tend])\n",
    "        in_surf_dim = len(features_surf)\n",
    "        \n",
    "        out_tend_dim = sum([get_var_dim(var) for var in target_tend])\n",
    "        out_surf_dim = len(target_surf)\n",
    "\n",
    "        return {\n",
    "            \"input_total\": in_tend_dim + in_surf_dim,\n",
    "            \"output_tendancies\": out_tend_dim,\n",
    "            \"output_surface\": out_surf_dim\n",
    "        }\n",
    "    \n",
    "class ClimSimKeras(ClimSimBase):\n",
    "    def get_batch_for_keras(self, idx):\n",
    "        x_np, y_np = self._prepare_data(idx)\n",
    "        # Ajout de la dimension Batch (1, ncol, n_feat)\n",
    "        return np.expand_dims(x_np, axis=0), np.expand_dims(y_np, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3a5d7f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: '/media/alexandre-tonon/UBUNTU 24_0/articleDL/data/ClimSim_low-res.zarr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/keras/lib/python3.8/site-packages/xarray/backends/zarr.py:400\u001b[0m, in \u001b[0;36mZarrStore.open_group\u001b[0;34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, stacklevel, zarr_version)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 400\u001b[0m     zarr_group \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_consolidated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/keras/lib/python3.8/site-packages/zarr/convenience.py:1351\u001b[0m, in \u001b[0;36mopen_consolidated\u001b[0;34m(store, metadata_key, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;66;03m# setup metadata store\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m meta_store \u001b[38;5;241m=\u001b[39m \u001b[43mConsolidatedStoreClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;66;03m# pass through\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/keras/lib/python3.8/site-packages/zarr/storage.py:2947\u001b[0m, in \u001b[0;36mConsolidatedMetadataStore.__init__\u001b[0;34m(self, store, metadata_key)\u001b[0m\n\u001b[1;32m   2946\u001b[0m \u001b[38;5;66;03m# retrieve consolidated metadata\u001b[39;00m\n\u001b[0;32m-> 2947\u001b[0m meta \u001b[38;5;241m=\u001b[39m json_loads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetadata_key\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m   2949\u001b[0m \u001b[38;5;66;03m# check format of consolidated metadata\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/keras/lib/python3.8/site-packages/zarr/storage.py:1113\u001b[0m, in \u001b[0;36mDirectoryStore.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: '.zmetadata'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mGroupNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/keras/lib/python3.8/site-packages/xarray/backends/zarr.py:403\u001b[0m, in \u001b[0;36mZarrStore.open_group\u001b[0;34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, stacklevel, zarr_version)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     zarr_group \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to open Zarr store with consolidated metadata, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut successfully read with non-consolidated metadata. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    418\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/keras/lib/python3.8/site-packages/zarr/hierarchy.py:1532\u001b[0m, in \u001b[0;36mopen_group\u001b[0;34m(store, mode, cache_attrs, synchronizer, path, chunk_store, storage_options, zarr_version, meta_array)\u001b[0m\n\u001b[1;32m   1531\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m ContainsArrayError(path)\n\u001b[0;32m-> 1532\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m GroupNotFoundError(path)\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mGroupNotFoundError\u001b[0m: group not found at path ''",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m      3\u001b[0m NORM_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClimSim/preprocessing/normalizations\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m FEATURES \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m :{\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtendancies\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_state_t\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_state_q0001\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_state_ps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     }\n\u001b[1;32m     14\u001b[0m }\n\u001b[0;32m---> 17\u001b[0m \u001b[43mClimSimKeras\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZARR_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGRID_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNORM_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFEATURES\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m, in \u001b[0;36mClimSimBase.__init__\u001b[0;34m(self, zarr_path, grid_path, norm_path, features, normalize)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, zarr_path, grid_path, norm_path, features, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_zarr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzarr_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m features\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_flag \u001b[38;5;241m=\u001b[39m normalize\n",
      "File \u001b[0;32m~/anaconda3/envs/keras/lib/python3.8/site-packages/xarray/backends/zarr.py:824\u001b[0m, in \u001b[0;36mopen_zarr\u001b[0;34m(store, group, synchronizer, chunks, decode_cf, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, consolidated, overwrite_encoded_chunks, chunk_store, storage_options, decode_timedelta, use_cftime, zarr_version, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    811\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopen_zarr() got unexpected keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(kwargs\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    812\u001b[0m     )\n\u001b[1;32m    814\u001b[0m backend_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynchronizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: synchronizer,\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsolidated\u001b[39m\u001b[38;5;124m\"\u001b[39m: consolidated,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzarr_version\u001b[39m\u001b[38;5;124m\"\u001b[39m: zarr_version,\n\u001b[1;32m    822\u001b[0m }\n\u001b[0;32m--> 824\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_cf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_cf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzarr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cftime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/anaconda3/envs/keras/lib/python3.8/site-packages/xarray/backends/api.py:541\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    530\u001b[0m     decode_cf,\n\u001b[1;32m    531\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    538\u001b[0m )\n\u001b[1;32m    540\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 541\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    548\u001b[0m     backend_ds,\n\u001b[1;32m    549\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    558\u001b[0m )\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/anaconda3/envs/keras/lib/python3.8/site-packages/xarray/backends/zarr.py:887\u001b[0m, in \u001b[0;36mZarrBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, synchronizer, consolidated, chunk_store, storage_options, stacklevel, zarr_version)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    868\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    883\u001b[0m     zarr_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    884\u001b[0m ):\n\u001b[1;32m    886\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 887\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mZarrStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacklevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m~/anaconda3/envs/keras/lib/python3.8/site-packages/xarray/backends/zarr.py:420\u001b[0m, in \u001b[0;36mZarrStore.open_group\u001b[0;34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, stacklevel, zarr_version)\u001b[0m\n\u001b[1;32m    404\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    405\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to open Zarr store with consolidated metadata, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut successfully read with non-consolidated metadata. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    418\u001b[0m             )\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m zarr\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mGroupNotFoundError:\n\u001b[0;32m--> 420\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m consolidated:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# TODO: an option to pass the metadata_key keyword\u001b[39;00m\n\u001b[1;32m    423\u001b[0m     zarr_group \u001b[38;5;241m=\u001b[39m zarr\u001b[38;5;241m.\u001b[39mopen_consolidated(store, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: '/media/alexandre-tonon/UBUNTU 24_0/articleDL/data/ClimSim_low-res.zarr'"
     ]
    }
   ],
   "source": [
    "ZARR_PATH = \"/media/alexandre-tonon/UBUNTU 24_0/articleDL/data/ClimSim_low-res.zarr\"\n",
    "GRID_PATH = \"data/ClimSim_low-res/ClimSim_low-res_grid-info.nc\"\n",
    "NORM_PATH = \"ClimSim/preprocessing/normalizations\"\n",
    "\n",
    "FEATURES = {\n",
    "    \"features\" :{\n",
    "        \"tendancies\" : [\"in_state_t\", \"in_state_q0001\", \"in_state_ps\"],\n",
    "        \"surface\" : [\"in_pbuf_LHFLX\", \"in_pbuf_SHFLX\", \"in_pbuf_SOLIN\"],\n",
    "    },  \n",
    "    \"target\" :{\n",
    "        \"tendancies\" : [\"out_ptend_t\", \"out_ptend_q0001\"],\n",
    "        \"surface\" : [\"out_cam_out_NETSW\", \"out_cam_out_FLWDS\", \"out_cam_out_PRECSC\", \"out_cam_out_PRECC\", \"out_cam_out_SOLS\", \"out_cam_out_SOLL\", \"out_cam_out_SOLSD\", \"out_cam_out_SOLLD\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "ClimSimKeras(ZARR_PATH, GRID_PATH, NORM_PATH, FEATURES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
