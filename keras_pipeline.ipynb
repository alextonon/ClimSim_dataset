{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "821e660f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandre-tonon/anaconda3/envs/pie_env/lib/python3.12/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"retrained_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"retrained_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">96,000</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">492,160</span> │ leaky_re_lu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">328,192</span> │ leaky_re_lu_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">328,320</span> │ leaky_re_lu_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">410,240</span> │ leaky_re_lu_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ leaky_re_lu_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">15,480</span> │ leaky_re_lu_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │ leaky_re_lu_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │     \u001b[38;5;34m96,000\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m)       │    \u001b[38;5;34m492,160\u001b[0m │ leaky_re_lu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m328,192\u001b[0m │ leaky_re_lu_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m)       │    \u001b[38;5;34m328,320\u001b[0m │ leaky_re_lu_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m)       │    \u001b[38;5;34m410,240\u001b[0m │ leaky_re_lu_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m82,048\u001b[0m │ leaky_re_lu_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)       │     \u001b[38;5;34m15,480\u001b[0m │ leaky_re_lu_5[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │      \u001b[38;5;34m1,032\u001b[0m │ leaky_re_lu_5[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,753,472</span> (6.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,753,472\u001b[0m (6.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,753,472</span> (6.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,753,472\u001b[0m (6.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "CNN_PATH = \"ClimSim/baseline_models/CNN/model/\"\n",
    "MLP_PATH = \"lib/ClimSim/baseline_models/MLP/model/backup_phase-7_retrained_models_step2_lot-147_trial_0027.best.h5\"\n",
    "\n",
    "# cnn_model = tf.keras.layers.TFSMLayer(CNN_PATH, call_endpoint=\"serving_default\")\n",
    "mlp_model = tf.keras.models.load_model(MLP_PATH, compile=False)\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86f9e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils.data import Dataset, Subset\n",
    "import re\n",
    "\n",
    "from lib import data\n",
    "\n",
    "\n",
    "class ClimSimBase:\n",
    "    def __init__(self, zarr_path, grid_path, norm_path, features, num_latlon = 384, normalize=True):\n",
    "        self.ds = xr.open_zarr(zarr_path, chunks=None)\n",
    "        self.features = features\n",
    "        self.features_list = self.__get_features__()\n",
    "        self.normalize_flag = normalize\n",
    "        self.normalize = normalize  \n",
    "        self.num_latlon = num_latlon\n",
    "        self.grid = xr.open_dataset(grid_path, engine=\"netcdf4\")\n",
    "        \n",
    "        self.input_mean = xr.open_dataset(os.path.join(norm_path, \"inputs/input_mean.nc\"), engine=\"h5netcdf\")\n",
    "        self.input_std = xr.open_dataset(os.path.join(norm_path, \"inputs/input_std.nc\"), engine=\"h5netcdf\")\n",
    "        self.input_max = xr.open_dataset(os.path.join(norm_path, \"inputs/input_max.nc\"), engine=\"h5netcdf\")\n",
    "        self.input_min = xr.open_dataset(os.path.join(norm_path, \"inputs/input_min.nc\"), engine=\"h5netcdf\")\n",
    "        self.output_scale = xr.open_dataset(os.path.join(norm_path, \"outputs/output_scale.nc\"), engine=\"h5netcdf\")\n",
    "\n",
    "        self.grid['area_wgt'] = self.grid['area']/self.grid['area'].mean(dim = 'ncol')\n",
    "        self.area_wgt = self.grid['area_wgt'].values\n",
    "\n",
    "        self.input_vars = [v for v in self.features_list if 'in' in v]\n",
    "        self.output_vars = [v for v in self.features_list if 'out' in v]\n",
    "\n",
    "        self.grav    = 9.80616    # acceleration of gravity ~ m/s^2\n",
    "        self.cp      = 1.00464e3  # specific heat of dry air   ~ J/kg/K\n",
    "        self.lv      = 2.501e6    # latent heat of evaporation ~ J/kg\n",
    "        self.lf      = 3.337e5    # latent heat of fusion      ~ J/kg\n",
    "        self.lsub    = self.lv + self.lf    # latent heat of sublimation ~ J/kg\n",
    "        self.rho_air = 101325/(6.02214e26*1.38065e-23/28.966)/273.15 # density of dry air at STP  ~ kg/m^3\n",
    "        self.rho_h20 = 1.e3       # density of fresh water     ~ kg/m^ 3\n",
    "\n",
    "        self.target_energy_conv = {'ptend_t':self.cp,\n",
    "                            'ptend_q0001':self.lv,\n",
    "                            'ptend_q0002':self.lv,\n",
    "                            'ptend_q0003':self.lv,\n",
    "                            'ptend_qn':self.lv,\n",
    "                            'ptend_wind': None,\n",
    "                            'cam_out_NETSW':1.,\n",
    "                            'cam_out_FLWDS':1.,\n",
    "                            'cam_out_PRECSC':self.lv*self.rho_h20,\n",
    "                            'cam_out_PRECC':self.lv*self.rho_h20,\n",
    "                            'cam_out_SOLS':1.,\n",
    "                            'cam_out_SOLL':1.,\n",
    "                            'cam_out_SOLSD':1.,\n",
    "                            'cam_out_SOLLD':1.\n",
    "                            }\n",
    "        \n",
    "        \n",
    "        self.dp = None \n",
    "        self.pressure_grid = None\n",
    "\n",
    "    def __get_features__(self):\n",
    "        feat = np.concatenate([self.features[\"features\"][\"multilevel\"], self.features[\"features\"][\"surface\"]])\n",
    "        target = np.concatenate([self.features[\"target\"][\"tendancies\"], self.features[\"target\"][\"surface\"]])\n",
    "        return np.concatenate([feat, target])\n",
    "\n",
    "    def _prepare_data(self, idx):\n",
    "        x = self.process_list(self.input_vars, idx, is_input=True)\n",
    "        y = self.process_list(self.output_vars, idx, is_input=False)\n",
    "        return x, y\n",
    "\n",
    "    def process_list(self, vars_list, idx, is_input=True):\n",
    "        out_list = []\n",
    "        n_geo = self.num_latlon # 384\n",
    "\n",
    "        for var in vars_list:\n",
    "            if \"ptend\" in var:\n",
    "                # Cette fonction doit renvoyer du (Time, 384, 60)\n",
    "                data = self._calculate_tendency_on_fly(var, idx)\n",
    "                if data.ndim == 2: # Si (384, 60)\n",
    "                    data = data[np.newaxis, :, :]\n",
    "            \n",
    "            else:\n",
    "                da = self.ds[var].isel(sample=idx)\n",
    "                \n",
    "                # Redressement par nom de dimension Xarray\n",
    "                if 'lev' in da.dims:\n",
    "                    if \"sample\" in da.dims:\n",
    "                        data = da.transpose('sample', 'ncol', 'lev').values\n",
    "                    else:\n",
    "                        data = da.transpose('ncol', 'lev').values[np.newaxis, :, :]\n",
    "                else:\n",
    "                    if \"sample\" in da.dims:\n",
    "                        # Surface : (Time, 384)\n",
    "                        data = da.values[:, :, np.newaxis]  # Ajouter une dimension lev=1\n",
    "                    else:\n",
    "                        # Surface : (Time, 384) -> (Time, 384, 1)\n",
    "                        data = da.values[np.newaxis, :, np.newaxis]\n",
    "            \n",
    "            # 2. Normalisation (Maintenant data est garanti (N, 384, L))\n",
    "            data = self._normalize_var(data, var, is_input=is_input)\n",
    "            out_list.append(data.astype(np.float32))\n",
    "\n",
    "        # 3. Concaténation et aplatissement\n",
    "        combined = np.concatenate(out_list, axis=-1)\n",
    "        return combined.reshape(-1, combined.shape[-1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ds.dims['sample']\n",
    "\n",
    "    def _calculate_tendency_on_fly(self, var, idx):\n",
    "        dt = 1200\n",
    "        mapping = {\n",
    "            'out_ptend_t': ('out_state_t', 'in_state_t'),\n",
    "            'out_ptend_q0001': ('out_state_q0001', 'in_state_q0001'),\n",
    "            'out_ptend_u': ('out_state_u', 'in_state_u'),\n",
    "            'out_ptend_v': ('out_state_v', 'in_state_v'),\n",
    "        }\n",
    "        out_v, in_v = mapping[var]\n",
    "\n",
    "        v_final = self.ds[out_v].isel(sample=idx)\n",
    "        v_init  = self.ds[in_v].isel(sample=idx)\n",
    "\n",
    "        # Fonction utilitaire: remettre en ordre (sample?, ncol, lev) si sample existe\n",
    "        def to_array(da):\n",
    "            dims = da.dims\n",
    "\n",
    "            if 'ncol' not in dims or 'lev' not in dims:\n",
    "                raise ValueError(f\"{da.name}: dims inattendues {dims}, attendu ncol & lev\")\n",
    "\n",
    "            # Cas slice -> dims contiennent sample\n",
    "            if 'sample' in dims:\n",
    "                da = da.transpose('sample', 'ncol', 'lev')\n",
    "                return da.values  # (sample, ncol, lev)\n",
    "\n",
    "            # Cas int -> dims (lev, ncol) ou (ncol, lev)\n",
    "            da = da.transpose('ncol', 'lev')\n",
    "            return da.values[None, ...]  # (1, ncol, lev)\n",
    "\n",
    "        vf = to_array(v_final)\n",
    "        vi = to_array(v_init)\n",
    "\n",
    "        return (vf - vi) / dt  # (time, ncol, lev)\n",
    "\n",
    "\n",
    "    \n",
    "    def _normalize_var(self, data, var_name, is_input=True):\n",
    "        # data est (N, 384, L) où L est 1 ou 60\n",
    "        short_name = re.sub(r'^(in_|out_)', '', var_name)\n",
    "        \n",
    "        if is_input:\n",
    "            m = self.input_mean[short_name].values     # (L,)\n",
    "            diff = (self.input_max[short_name].values - self.input_min[short_name].values) # (L,)\n",
    "            \n",
    "            # On redimensionne les stats en (1, 1, L) pour s'aligner sur data (N, 384, L)\n",
    "            m = m.reshape(1, 1, -1)\n",
    "            diff = diff.reshape(1, 1, -1)\n",
    "            \n",
    "            return (data - m) / (diff + 1e-15)\n",
    "        else:\n",
    "            scale = self.output_scale[short_name].values # (L,)\n",
    "            return data * scale.reshape(1, 1, -1)\n",
    "            \n",
    "    def set_pressure_grid(self, input_data):\n",
    "        '''\n",
    "        Calcule la grille de pression 3D à partir de state_ps.\n",
    "        Code directement issu de ClimSim original.\n",
    "        '''\n",
    "        self.ps_index = self._find_ps_index(self.features)\n",
    "        state_ps = input_data[:, self.ps_index]\n",
    "        if self.normalize_flag:\n",
    "            state_ps = state_ps * (self.input_max['state_ps'].values - self.input_min['state_ps'].values) + self.input_mean['state_ps'].values\n",
    "\n",
    "        state_ps = state_ps.reshape(-1, self.num_latlon)\n",
    "\n",
    "        p1 = (self.grid['P0'] * self.grid['hyai']).values[:, None, None]\n",
    "        p2 = self.grid['hybi'].values[:, None, None] * state_ps[None, :, :]\n",
    "        \n",
    "        self.pressure_grid = p1 + p2\n",
    "        self.dp = (self.pressure_grid[1:61] - self.pressure_grid[0:60]).transpose((1, 2, 0))\n",
    "    \n",
    "    \n",
    "    def denormalize_output(self, y_pred):\n",
    "        \"\"\"Dénormalise les prédictions.\"\"\"\n",
    "\n",
    "        full_scale_vector = [] # To vectorize we generate the full scale vector first\n",
    "        for var in self.output_vars:\n",
    "            short_name = re.sub(r'^(in_|out_)', '', var)\n",
    "            scale = self.output_scale[short_name].values\n",
    "            \n",
    "            if 'ptend' in var:\n",
    "                dim_size = 60\n",
    "            elif 'lev' in self.ds[var].dims:\n",
    "                dim_size = self.ds[var].sizes['lev']\n",
    "            else:\n",
    "                dim_size = 1\n",
    "            \n",
    "            if np.isscalar(scale) or scale.size == 1:\n",
    "                scale_expanded = np.full(dim_size, scale)\n",
    "            else:\n",
    "                scale_expanded = scale\n",
    "                \n",
    "            full_scale_vector.append(scale_expanded)\n",
    "        \n",
    "        full_scale_vector = np.concatenate(full_scale_vector)\n",
    "\n",
    "        y_denorm = y_pred / (full_scale_vector + 1e-8) \n",
    "\n",
    "        return (y_denorm).astype(np.float32)\n",
    "    \n",
    "    def calc_MAE(self, pred, target, avg_grid = True):\n",
    "        '''\n",
    "        calculate 'globally averaged' mean absolute error \n",
    "        for vertically-resolved variables, shape should be time x grid x level\n",
    "        for scalars, shape should be time x grid\n",
    "\n",
    "        returns vector of length level or 1\n",
    "        '''\n",
    "        assert pred.shape[1] == self.num_latlon\n",
    "        assert pred.shape == target.shape\n",
    "        mae = np.abs(pred - target).mean(axis = 0)\n",
    "        if avg_grid:\n",
    "            return mae.mean(axis = 0) # we decided to average globally at end\n",
    "        else:\n",
    "            return mae\n",
    "    \n",
    "    def calc_RMSE(self, pred, target, avg_grid = True):\n",
    "        '''\n",
    "        calculate 'globally averaged' root mean squared error \n",
    "        for vertically-resolved variables, shape should be time x grid x level\n",
    "        for scalars, shape should be time x grid\n",
    "\n",
    "        returns vector of length level or 1\n",
    "        '''\n",
    "        assert pred.shape[1] == self.num_latlon\n",
    "        assert pred.shape == target.shape\n",
    "        sq_diff = (pred - target)**2\n",
    "        rmse = np.sqrt(sq_diff.mean(axis = 0)) # mean over time\n",
    "        if avg_grid:\n",
    "            return rmse.mean(axis = 0) # we decided to separately average globally at end\n",
    "        else:\n",
    "            return rmse\n",
    "\n",
    "    def calc_R2(self, pred, target, avg_grid = True):\n",
    "        '''\n",
    "        calculate 'globally averaged' R-squared\n",
    "        for vertically-resolved variables, input shape should be time x grid x level\n",
    "        for scalars, input shape should be time x grid\n",
    "\n",
    "        returns vector of length level or 1\n",
    "        '''\n",
    "        assert pred.shape[1] == self.num_latlon\n",
    "        assert pred.shape == target.shape\n",
    "        sq_diff = (pred - target)**2\n",
    "        tss_time = (target - target.mean(axis = 0)[np.newaxis, ...])**2 # mean over time\n",
    "        r_squared = 1 - sq_diff.sum(axis = 0)/tss_time.sum(axis = 0) # sum over time\n",
    "        if avg_grid:\n",
    "            return r_squared.mean(axis = 0) # we decided to separately average globally at end\n",
    "        else:\n",
    "            return r_squared\n",
    "    \n",
    "    def output_weighting(self, output, just_weights=False):\n",
    "        num_samples = output.shape[0]\n",
    "        n_geo = self.num_latlon\n",
    "        n_time = num_samples // n_geo\n",
    "        \n",
    "        # Configuration des indices : (début, fin, est_profil)\n",
    "        offsets = {\n",
    "            'ptend_t': (0, 60, True), 'ptend_q0001': (60, 120, True),\n",
    "            'cam_out_NETSW': (120, 121, False), 'cam_out_FLWDS': (121, 122, False),\n",
    "            'cam_out_PRECSC': (122, 123, False), 'cam_out_PRECC': (123, 124, False),\n",
    "            'cam_out_SOLS': (124, 125, False), 'cam_out_SOLL': (125, 126, False),\n",
    "            'cam_out_SOLSD': (126, 127, False), 'cam_out_SOLLD': (127, 128, False)\n",
    "        }\n",
    "        \n",
    "        dp = self.dp / self.grav\n",
    "        var_dict = {}\n",
    "\n",
    "        for var, (start, end, is_prof) in offsets.items():\n",
    "            # Extraction et reshape\n",
    "            val = output[:, start:end].reshape(n_time, n_geo, -1 if is_prof else 1)\n",
    "            if not is_prof: val = val.squeeze(-1)\n",
    "\n",
    "            # [0] Undo scaling\n",
    "            scale = self.output_scale[var].values\n",
    "            val /= scale[None, None, :] if is_prof else scale\n",
    "\n",
    "            # [1] Vertical weighting\n",
    "            if is_prof: val *= dp\n",
    "            \n",
    "            # [2] Area weighting\n",
    "            val *= self.area_wgt[None, :, None] if is_prof else self.area_wgt[None, :]\n",
    "            \n",
    "            # [3] Energy conversion\n",
    "            val *= self.target_energy_conv[var]\n",
    "            \n",
    "            \n",
    "            var_dict[var] = val\n",
    "\n",
    "        return var_dict\n",
    "    \n",
    "    def _find_ps_index(self, features_dict):\n",
    "        \"\"\"\n",
    "        Calcule l'index de départ de 'in_state_ps' dans le vecteur d'entrée plat.\n",
    "        Prend en compte que chaque variable multilevel occupe 60 colonnes.\n",
    "        \"\"\"\n",
    "        current_index = 0\n",
    "        \n",
    "        # 1. Parcourir les variables multi-niveaux (60 niveaux chacune)\n",
    "        for var in features_dict[\"features\"][\"multilevel\"]:\n",
    "            if var == \"in_state_ps\":\n",
    "                return current_index\n",
    "            current_index += 60\n",
    "            \n",
    "        # 2. Parcourir les variables de surface (1 niveau chacune)\n",
    "        for var in features_dict[\"features\"][\"surface\"]:\n",
    "            if var == \"in_state_ps\":\n",
    "                return current_index\n",
    "            current_index += 1\n",
    "            \n",
    "        raise ValueError(\"Variable 'in_state_ps' non trouvée dans le dictionnaire FEATURES.\")\n",
    "\n",
    "class ClimSimPyTorch(ClimSimBase, Dataset):\n",
    "    def __getitem__(self, idx):\n",
    "        x_np, y_np = self._prepare_data(idx)\n",
    "        return torch.from_numpy(x_np), torch.from_numpy(y_np)\n",
    "\n",
    "    # On peut remettre ta méthode de split ici\n",
    "    def train_test_split(self, test_size=0.2, seed=42, shuffle=True):\n",
    "        n = len(self)\n",
    "        indices = np.arange(n)\n",
    "        if shuffle:\n",
    "            rng = np.random.default_rng(seed)\n",
    "            rng.shuffle(indices)\n",
    "        split = int((1 - test_size) * n)\n",
    "        return Subset(self, indices[:split]), Subset(self, indices[split:])\n",
    "    \n",
    "    def get_models_dims(self, variables_dict):\n",
    "        features_tend = variables_dict[\"features\"][\"multilevel\"]\n",
    "        features_surf = variables_dict[\"features\"][\"surface\"]\n",
    "        \n",
    "        target_tend = variables_dict[\"target\"][\"tendancies\"]\n",
    "        target_surf = variables_dict[\"target\"][\"surface\"]\n",
    "\n",
    "        def get_var_dim(var):\n",
    "            # 1. Gérer les variables virtuelles (tendances calculées)\n",
    "            if 'ptend' in var:\n",
    "                # On mappe vers la variable d'état pour connaître la dimension 'lev'\n",
    "                # ex: out_ptend_t -> out_state_t\n",
    "                source_var = var.replace('ptend', 'state')\n",
    "                return self.ds[source_var].sizes['lev']\n",
    "            \n",
    "            # 2. Gérer les variables réelles présentes dans le Zarr\n",
    "            if 'lev' in self.ds[var].dims:\n",
    "                return self.ds[var].sizes['lev']\n",
    "            \n",
    "            # 3. Variables de surface (scalaires)\n",
    "            return 1\n",
    "\n",
    "        in_tend_dim = sum([get_var_dim(var) for var in features_tend])\n",
    "        in_surf_dim = len(features_surf)\n",
    "        \n",
    "        out_tend_dim = sum([get_var_dim(var) for var in target_tend])\n",
    "        out_surf_dim = len(target_surf)\n",
    "\n",
    "        return {\n",
    "            \"input_total\": in_tend_dim + in_surf_dim,\n",
    "            \"output_tendancies\": out_tend_dim,\n",
    "            \"output_surface\": out_surf_dim\n",
    "        }\n",
    "            \n",
    "class ClimSimKeras(ClimSimBase):\n",
    "    def get_batch_for_keras(self, idx, batch_size, input_dim, output_dim):\n",
    "        in_batch = np.zeros((batch_size, self.num_latlon, input_dim), dtype=np.float32)\n",
    "        out_batch = np.zeros((batch_size, self.num_latlon, output_dim), dtype=np.float32)\n",
    "\n",
    "        for s in range(batch_size):\n",
    "            if idx * batch_size + s >= self.get_sample_number():\n",
    "                raise IndexError(\"Index hors limites pour le dataset.\")\n",
    "        \n",
    "            x_np, y_np = self._prepare_data(idx * batch_size + s)\n",
    "\n",
    "            in_batch[s, :, :] = x_np.reshape(self.num_latlon, input_dim)\n",
    "            out_batch[s, :, :] = y_np.reshape(self.num_latlon, output_dim)\n",
    "        return in_batch, out_batch\n",
    "    \n",
    "    def get_sample_number(self):\n",
    "        return self.ds.dims['sample']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3a5d7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input/output dimensions: {'input_total': 124, 'output_tendancies': 120, 'output_surface': 8}\n"
     ]
    }
   ],
   "source": [
    "ZARR_PATH = \"data/ClimSim_low-res.zarr\"\n",
    "GRID_PATH = \"data/ClimSim_low-res/ClimSim_low-res_grid-info.nc\"\n",
    "NORM_PATH = \"lib/ClimSim/preprocessing/normalizations\"\n",
    "\n",
    "FEATURES = {\n",
    "    \"features\" :{\n",
    "        \"multilevel\" : [\"in_state_t\", \"in_state_q0001\"],\n",
    "        \"surface\" : [ \"in_state_ps\", 'in_pbuf_SOLIN', 'in_pbuf_LHFLX', 'in_pbuf_SHFLX'],\n",
    "    },  \n",
    "    \"target\" :{\n",
    "        \"tendancies\" : [\"out_ptend_t\", \"out_ptend_q0001\"],\n",
    "        \"surface\" : [\"out_cam_out_NETSW\", \"out_cam_out_FLWDS\", \"out_cam_out_PRECSC\", \"out_cam_out_PRECC\", \"out_cam_out_SOLS\", \"out_cam_out_SOLL\", \"out_cam_out_SOLSD\", \"out_cam_out_SOLLD\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "models_dims = ClimSimPyTorch(ZARR_PATH, GRID_PATH, NORM_PATH, FEATURES).get_models_dims(FEATURES)\n",
    "print(\"Model input/output dimensions:\", models_dims)\n",
    "\n",
    "dataset = ClimSimKeras(ZARR_PATH, GRID_PATH, NORM_PATH, FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd1f3adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 384, 124) (2, 384, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27772/2068524723.py:386: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  return self.ds.dims['sample']\n"
     ]
    }
   ],
   "source": [
    "target_vars = FEATURES[\"target\"][\"tendancies\"] + FEATURES[\"target\"][\"surface\"]\n",
    "input_size = models_dims[\"input_total\"]\n",
    "output_size = models_dims[\"output_tendancies\"] + models_dims[\"output_surface\"]\n",
    "\n",
    "\n",
    "x_batch, y_true_batch = dataset.get_batch_for_keras(1,2, input_size, output_size)\n",
    "print(x_batch.shape, y_true_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "456f37eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27772/2068524723.py:386: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  return self.ds.dims['sample']\n",
      "/tmp/ipykernel_27772/2068524723.py:254: RuntimeWarning: divide by zero encountered in divide\n",
      "  r_squared = 1 - sq_diff.sum(axis = 0)/tss_time.sum(axis = 0) # sum over time\n",
      "/tmp/ipykernel_27772/2068524723.py:254: RuntimeWarning: invalid value encountered in divide\n",
      "  r_squared = 1 - sq_diff.sum(axis = 0)/tss_time.sum(axis = 0) # sum over time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variable                  | MAE Moyenne (W/m2)\n",
      "---------------------------------------------\n",
      "ptend_t                   | 9.8517e+00\n",
      "ptend_q0001               | 8.2674e+00\n",
      "cam_out_NETSW             | 3.7105e+01\n",
      "cam_out_FLWDS             | 5.7761e+00\n",
      "cam_out_PRECSC            | 3.1328e+00\n",
      "cam_out_PRECC             | 3.0229e+01\n",
      "cam_out_SOLS              | 1.5977e+01\n",
      "cam_out_SOLL              | 1.9004e+01\n",
      "cam_out_SOLSD             | 8.7456e+00\n",
      "cam_out_SOLLD             | 7.0313e+00\n",
      "ptend_t                   | -inf\n",
      "ptend_q0001               | -inf\n",
      "cam_out_NETSW             | nan\n",
      "cam_out_FLWDS             | -inf\n",
      "cam_out_PRECSC            | nan\n",
      "cam_out_PRECC             | nan\n",
      "cam_out_SOLS              | nan\n",
      "cam_out_SOLL              | nan\n",
      "cam_out_SOLSD             | nan\n",
      "cam_out_SOLLD             | nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "target_vars = FEATURES[\"target\"][\"tendancies\"] + FEATURES[\"target\"][\"surface\"]\n",
    "input_size = models_dims[\"input_total\"]\n",
    "output_size = models_dims[\"output_tendancies\"] + models_dims[\"output_surface\"]\n",
    "\n",
    "# 1. Initialisation des accumulateurs\n",
    "stats = {}\n",
    "for var in target_vars:\n",
    "    short_name = re.sub(r'^(in_|out_)', '', var)\n",
    "    size = 60 if var in FEATURES[\"target\"][\"tendancies\"] else 1\n",
    "    stats[short_name] = {\n",
    "        \"ss_res\": np.zeros(size),      # Somme des carrés (pour RMSE et R2)\n",
    "        \"sum_abs_err\": np.zeros(size), # Somme des erreurs absolues (pour MAE)\n",
    "        \"sum_y\": np.zeros(size),       # Somme de y (pour variance R2)\n",
    "        \"sum_y_sq\": np.zeros(size),    # Somme de y^2 (pour variance R2)\n",
    "        \"count\": 0\n",
    "    }\n",
    "\n",
    "all_mae = {short_name: [] for short_name in stats.keys()}\n",
    "all_r2 = {short_name: [] for short_name in stats.keys()}\n",
    "\n",
    "# 2. Boucle de test (Batch par Batch)\n",
    "for i in range(10):\n",
    "    x_batch, y_true_batch = dataset.get_batch_for_keras(i, batch_size=1, input_dim=input_size, output_dim=output_size)\n",
    "\n",
    "    x_flat = x_batch.reshape([-1, x_batch.shape[2]])\n",
    "    y_flat = y_true_batch.reshape([-1, y_true_batch.shape[2]])\n",
    "    \n",
    "    dataset.set_pressure_grid(x_flat)\n",
    "    preds_batch = mlp_model.predict(x_flat, verbose=0)\n",
    "    \n",
    "    true_dict = dataset.output_weighting(y_flat)\n",
    "    pred_dict = dataset.output_weighting(preds_batch)\n",
    "    \n",
    "    for var in target_vars:\n",
    "        short_name = re.sub(r'^(in_|out_)', '', var)\n",
    "        y_t = true_dict[short_name] \n",
    "        y_p = pred_dict[short_name]\n",
    "        \n",
    "        mae_score = dataset.calc_MAE(y_p, y_t, avg_grid=True)\n",
    "        all_mae[short_name].append(mae_score)\n",
    "\n",
    "        r2_score = dataset.calc_R2(y_p, y_t, avg_grid=True)\n",
    "        all_r2[short_name].append(r2_score)\n",
    "\n",
    "\n",
    "# 2. Affichage des moyennes finales\n",
    "print(f\"\\n{'Variable':<25} | {'MAE Moyenne (W/m2)':<15}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for var, scores in all_mae.items():\n",
    "    # Moyenne sur tous les batches, puis moyenne sur les 60 niveaux\n",
    "    final_score = np.mean(np.mean(scores, axis=0))\n",
    "    print(f\"{var:<25} | {final_score:.4e}\")\n",
    "\n",
    "for var, scores in all_r2.items():\n",
    "    # Moyenne sur tous les batches, puis moyenne sur les 60 niveaux\n",
    "    final_score = np.mean(np.mean(scores, axis=0))\n",
    "    print(f\"{var:<25} | {final_score:.4e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdd15770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27772/2068524723.py:386: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  return self.ds.dims['sample']\n",
      "2026-02-01 17:46:37.103315: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 19660800 exceeds 10% of free system memory.\n",
      "2026-02-01 17:46:45.656941: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 19660800 exceeds 10% of free system memory.\n",
      "2026-02-01 17:46:54.051723: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 19660800 exceeds 10% of free system memory.\n",
      "2026-02-01 17:47:02.617500: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 19660800 exceeds 10% of free system memory.\n",
      "2026-02-01 17:47:11.219945: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 19660800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SCORES GLOBAUX (dataset-level) ===\n",
      "Variable                  |   MAE global |    R2 global\n",
      "---------------------------------------------------------\n",
      "ptend_t                   |   1.1387e+01 |   6.7116e-02\n",
      "ptend_q0001               |   9.1280e+00 |  -2.5178e+03\n",
      "cam_out_NETSW             |   3.9877e+01 |   8.8796e-01\n",
      "cam_out_FLWDS             |   5.7493e+00 |   9.9110e-01\n",
      "cam_out_PRECSC            |   2.8807e+00 |   8.5202e-01\n",
      "cam_out_PRECC             |   3.5260e+01 |   7.7384e-01\n",
      "cam_out_SOLS              |   1.7661e+01 |   8.7417e-01\n",
      "cam_out_SOLL              |   1.9969e+01 |   8.6958e-01\n",
      "cam_out_SOLSD             |   9.2597e+00 |   8.7935e-01\n",
      "cam_out_SOLLD             |   7.0668e+00 |   8.2339e-01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# ---------------------------\n",
    "# Config\n",
    "# ---------------------------\n",
    "target_vars = FEATURES[\"target\"][\"tendancies\"] + FEATURES[\"target\"][\"surface\"]\n",
    "input_size = models_dims[\"input_total\"]\n",
    "output_size = models_dims[\"output_tendancies\"] + models_dims[\"output_surface\"]\n",
    "\n",
    "# Pour savoir la taille attendue (60 niveaux pour tendancies, 1 pour surface)\n",
    "def var_size(var: str) -> int:\n",
    "    return 60 if var in FEATURES[\"target\"][\"tendancies\"] else 1\n",
    "\n",
    "def short_name(var: str) -> str:\n",
    "    return re.sub(r'^(in_|out_)', '', var)\n",
    "\n",
    "def to_2d(y: np.ndarray, L: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Force y à une shape (N, L).\n",
    "    - Si y est scalaire ou (N,), L doit être 1 => (N, 1)\n",
    "    - Si y est déjà (N, L) OK\n",
    "    - Si y a plus de dims, on aplati tout sauf la dernière dim\n",
    "    \"\"\"\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    if L == 1:\n",
    "        # Tout ramener à (N, 1)\n",
    "        if y.ndim == 0:\n",
    "            return y.reshape(1, 1)\n",
    "        if y.ndim == 1:\n",
    "            return y.reshape(-1, 1)\n",
    "        # si y est (N, 1) ou (.., 1) : aplatis tout sauf last\n",
    "        return y.reshape(-1, 1)\n",
    "\n",
    "    # L > 1 (ex: 60)\n",
    "    if y.ndim == 1:\n",
    "        # Cas improbable: on attend L, mais on a (N,) -> erreur claire\n",
    "        raise ValueError(f\"Attendu un tableau avec {L} niveaux, reçu shape {y.shape}\")\n",
    "    if y.shape[-1] != L:\n",
    "        raise ValueError(f\"Attendu dernière dimension = {L}, reçu shape {y.shape}\")\n",
    "    return y.reshape(-1, L)\n",
    "\n",
    "# ---------------------------\n",
    "# Accumulateurs globaux\n",
    "# ---------------------------\n",
    "stats = {}\n",
    "for var in target_vars:\n",
    "    sname = short_name(var)\n",
    "    L = var_size(var)\n",
    "    stats[sname] = {\n",
    "        \"sum_abs_err\": np.zeros(L, dtype=np.float64),\n",
    "        \"ss_res\":      np.zeros(L, dtype=np.float64),\n",
    "        \"sum_y\":       np.zeros(L, dtype=np.float64),\n",
    "        \"sum_y_sq\":    np.zeros(L, dtype=np.float64),\n",
    "        \"count\":       0\n",
    "    }\n",
    "\n",
    "# Optionnel : garder aussi les scores \"moyenne par batch\" pour comparer\n",
    "mean_batch_r2 = {short_name(v): [] for v in target_vars}\n",
    "mean_batch_mae = {short_name(v): [] for v in target_vars}\n",
    "\n",
    "# ---------------------------\n",
    "# Boucle de test\n",
    "# ---------------------------\n",
    "n_batches = 10\n",
    "eps = 1e-12\n",
    "\n",
    "for i in range(n_batches):\n",
    "    x_batch, y_true_batch = dataset.get_batch_for_keras(\n",
    "        i, batch_size=100, input_dim=input_size, output_dim=output_size\n",
    "    )\n",
    "\n",
    "    # flatten (comme tu faisais)\n",
    "    x_flat = x_batch.reshape([-1, x_batch.shape[2]])\n",
    "    y_flat = y_true_batch.reshape([-1, y_true_batch.shape[2]])\n",
    "\n",
    "    dataset.set_pressure_grid(x_flat)\n",
    "    preds_batch = mlp_model.predict(x_flat, verbose=0)\n",
    "\n",
    "    true_dict = dataset.output_weighting(y_flat)\n",
    "    pred_dict = dataset.output_weighting(preds_batch)\n",
    "\n",
    "    for var in target_vars:\n",
    "        sname = short_name(var)\n",
    "        L = var_size(var)\n",
    "\n",
    "        y_t = to_2d(true_dict[sname], L)\n",
    "        y_p = to_2d(pred_dict[sname], L)\n",
    "\n",
    "        # --- accumulateurs globaux ---\n",
    "        err = (y_t - y_p)\n",
    "        stats[sname][\"sum_abs_err\"] += np.sum(np.abs(err), axis=0)\n",
    "        stats[sname][\"ss_res\"]      += np.sum(err**2, axis=0)\n",
    "        stats[sname][\"sum_y\"]       += np.sum(y_t, axis=0)\n",
    "        stats[sname][\"sum_y_sq\"]    += np.sum(y_t**2, axis=0)\n",
    "        stats[sname][\"count\"]       += y_t.shape[0]\n",
    "\n",
    "        # --- optionnel : tes métriques par batch (si tu veux comparer) ---\n",
    "        # Si tes fonctions renvoient vecteur (L,), on les stocke.\n",
    "        try:\n",
    "            mae_b = dataset.calc_MAE(y_p, y_t, avg_grid=True)\n",
    "            r2_b  = dataset.calc_R2(y_p, y_t, avg_grid=True)\n",
    "            mean_batch_mae[sname].append(np.asarray(mae_b))\n",
    "            mean_batch_r2[sname].append(np.asarray(r2_b))\n",
    "        except Exception:\n",
    "            # Si jamais calc_MAE/calc_R2 attend d'autres shapes, on ignore en silence.\n",
    "            pass\n",
    "\n",
    "# ---------------------------\n",
    "# Calculs finaux (GLOBAL)\n",
    "# ---------------------------\n",
    "print(\"\\n=== SCORES GLOBAUX (dataset-level) ===\")\n",
    "print(f\"{'Variable':<25} | {'MAE global':>12} | {'R2 global':>12}\")\n",
    "print(\"-\" * 57)\n",
    "\n",
    "for sname, s in stats.items():\n",
    "    N = s[\"count\"]\n",
    "    if N == 0:\n",
    "        print(f\"{sname:<25} | {'NA':>12} | {'NA':>12}\")\n",
    "        continue\n",
    "\n",
    "    mae_per_level = s[\"sum_abs_err\"] / N\n",
    "\n",
    "    y_mean = s[\"sum_y\"] / N\n",
    "    ss_tot = s[\"sum_y_sq\"] - N * (y_mean**2)\n",
    "\n",
    "    r2_per_level = 1.0 - (s[\"ss_res\"] / (ss_tot + eps))\n",
    "\n",
    "    # Un seul score par variable (moyenne sur niveaux)\n",
    "    mae_global = float(np.mean(mae_per_level))\n",
    "    r2_global  = float(np.mean(r2_per_level))\n",
    "\n",
    "    print(f\"{sname:<25} | {mae_global:12.4e} | {r2_global:12.4e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pie_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
