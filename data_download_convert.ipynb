{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d286a9",
   "metadata": {},
   "source": [
    "## Valiation dataset downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079a9ee3",
   "metadata": {},
   "source": [
    "The datasets are on hugging face.\n",
    "\n",
    "As models were not trained on the last years it was mandatory to download some samples to do a proper evaluation. However as they're too heavy, I __downloading 10% of the dataset__ making sure to download __pairs__ (I spent 10h of debugging after forgetting that...)\n",
    "\n",
    "<span style=\"color:red\">Do not launch this 100 minutes script</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b368cb0e",
   "metadata": {},
   "source": [
    "### Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from huggingface_hub import HfFileSystem, hf_hub_download\n",
    "\n",
    "fs = HfFileSystem()\n",
    "repo_id = \"LEAP/ClimSim_low-res\"\n",
    "local_dir = \"./public_data/ClimSim_low-res/\"\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "print(\"Recherche des fichiers mli...\")\n",
    "all_files = fs.glob(f\"datasets/{repo_id}/train/0008-*/*.mli.*.nc\")\n",
    "\n",
    "print(f\"{len(all_files)} fichiers mli trouv√©s.\")\n",
    "\n",
    "sample_size = int(0.01 * len(all_files)) \n",
    "sampled_mli = random.sample(all_files, sample_size)\n",
    "\n",
    "# 3. Reconstituer la liste finale avec les paires correspondantes (mli + mlo)\n",
    "final_list = []\n",
    "for mli_path in sampled_mli:\n",
    "    final_list.append(mli_path)\n",
    "    final_list.append(mli_path.replace(\".mli.\", \".mlo.\"))\n",
    "\n",
    "print(f\"Pr√™t √† t√©l√©charger {len(final_list)} fichiers.\")\n",
    "\n",
    "# 4. T√©l√©chargement direct\n",
    "for i, hf_path in enumerate(final_list):\n",
    "    filename = hf_path.replace(f\"datasets/{repo_id}/\", \"\")\n",
    "    print(f\"[{i+1}/{len(final_list)}] -> {filename}\")\n",
    "    \n",
    "    hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=filename,\n",
    "        repo_type=\"dataset\",\n",
    "        local_dir=local_dir,\n",
    "        local_dir_use_symlinks=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd50a3d9",
   "metadata": {},
   "source": [
    "### .zarr convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f308d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_paired_samples(path):\n",
    "    data_folders = sorted(os.listdir(path))\n",
    "    print(f\"Found {len(data_folders)} data folders.\")\n",
    "\n",
    "    mli_dict = {}\n",
    "    mlo_dict = {}\n",
    "\n",
    "    for dir_name in data_folders:\n",
    "        dir_path = os.path.join(path, dir_name)\n",
    "        if not os.path.isdir(dir_path):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(dir_path):\n",
    "            print(fname)\n",
    "            parts = fname.split('.')\n",
    "            \n",
    "            print(parts)\n",
    "            base = parts[0]        # identifiant commun\n",
    "            ext = parts[1]         # mli ou mlo\n",
    "            timestamp = parts[2]   # timestamp\n",
    "            full_path = os.path.join(dir_path, fname)\n",
    "\n",
    "            key = f\"{base}.{timestamp}\"\n",
    "\n",
    "            if ext == \"mli\":\n",
    "                mli_dict[key] = full_path\n",
    "            elif ext == \"mlo\":\n",
    "                mlo_dict[key] = full_path\n",
    "\n",
    "    # --- Cr√©ation des paires s√ªres ---\n",
    "    common_keys = sorted(set(mli_dict) & set(mlo_dict))\n",
    "    missing_mli = sorted(set(mlo_dict) - set(mli_dict))\n",
    "    missing_mlo = sorted(set(mli_dict) - set(mlo_dict))\n",
    "\n",
    "    print(f\"‚úÖ Paired samples: {len(common_keys)}\")\n",
    "    print(f\"‚ùå Missing MLI: {len(missing_mli)}\")\n",
    "    print(f\"‚ùå Missing MLO: {len(missing_mlo)}\")\n",
    "\n",
    "    if missing_mli:\n",
    "        print(\"Example missing MLI:\", missing_mli[:5])\n",
    "    if missing_mlo:\n",
    "        print(\"Example missing MLO:\", missing_mlo[:5])\n",
    "\n",
    "    pairs = [(mli_dict[k], mlo_dict[k]) for k in common_keys]\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def read_sample(file_path):\n",
    "    return xr.open_dataset(file_path)\n",
    "\n",
    "zarr_path = \"/home/alexandre-tonon/test/test_data/ClimSim_low-res_validation.zarr\"\n",
    "train_root = \"/home/alexandre-tonon/test/test_data/train\"\n",
    "\n",
    "# --- √âTAPE CRUCIALE : RESET ---\n",
    "if os.path.exists(zarr_path):\n",
    "    print(\"üßπ Nettoyage du Zarr corrompu...\")\n",
    "    shutil.rmtree(zarr_path)\n",
    "\n",
    "samples = get_paired_samples(train_root)\n",
    "\n",
    "chunk_size = 100\n",
    "chunk_number = (len(samples) + chunk_size - 1) // chunk_size\n",
    "\n",
    "for i in tqdm(range(chunk_number), desc=\"Progression totale\"):\n",
    "    buffer = []\n",
    "    current_samples = samples[i*chunk_size : (i+1)*chunk_size]\n",
    "    \n",
    "    for mli_path, mlo_path in tqdm(current_samples, desc=f\"Batch {i+1}\", leave=False):\n",
    "        # Utilisation de engine='netcdf4' ou 'h5netcdf'\n",
    "        with xr.open_dataset(mli_path, engine='netcdf4') as mli_ds, \\\n",
    "             xr.open_dataset(mlo_path, engine='netcdf4') as mlo_ds:\n",
    "\n",
    "            # On load en RAM pour casser le lien avec le fichier NetCDF\n",
    "            ds_i = mli_ds.rename({v: f\"in_{v}\" for v in mli_ds.data_vars}).load()\n",
    "            ds_o = mlo_ds.rename({v: f\"out_{v}\" for v in mlo_ds.data_vars}).load()\n",
    "\n",
    "            # Fusion et ajout manuel de la dimension 'sample'\n",
    "            merged = xr.merge([ds_i, ds_o], compat=\"override\").expand_dims(\"sample\")\n",
    "            buffer.append(merged)\n",
    "\n",
    "    if buffer:\n",
    "        ds_batch = xr.concat(buffer, dim=\"sample\")\n",
    "        \n",
    "        # On v√©rifie si c'est le TOUT PREMIER bloc √©crit dans cette session\n",
    "        if i == 0:\n",
    "            # Cr√©ation initiale (mode 'w')\n",
    "            ds_batch.to_zarr(zarr_path, mode=\"w\", consolidated=True)\n",
    "        else:\n",
    "            # Ajout (mode 'a')\n",
    "            ds_batch.to_zarr(zarr_path, mode=\"a\", append_dim=\"sample\", consolidated=True)\n",
    "        \n",
    "        # Nettoyage pour le prochain chunk\n",
    "        del ds_batch\n",
    "        import gc; gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
